import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from activation_steer import ActivationSteerer
import argparse

class PersonaChatbot:
    def __init__(self, model_name, vector_path, layer_idx=20, steering_coef=2.0):
        self.model_name = model_name
        self.layer_idx = layer_idx
        self.steering_coef = steering_coef
        
        # è¼‰å…¥æ¨¡å‹
        print(f"ğŸ¤– è¼‰å…¥æ¨¡å‹: {model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name, 
            device_map="auto",
            torch_dtype=torch.bfloat16
        )
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # è¼‰å…¥personaå‘é‡
        print(f"ğŸ“Š è¼‰å…¥å‘é‡: {vector_path}")
        self.persona_vectors = torch.load(vector_path, weights_only=False)
        
        self.conversation_history = []
    
    def set_steering_params(self, layer_idx=None, coef=None):
        """å‹•æ…‹èª¿æ•´steeringåƒæ•¸"""
        if layer_idx is not None:
            self.layer_idx = layer_idx
        if coef is not None:
            self.steering_coef = coef
        print(f"ğŸ”§ æ›´æ–°åƒæ•¸: layer={self.layer_idx}, coef={self.steering_coef}")
    
    # TODO: çœ‹ max_token æ˜¯å¦éœ€è¦ä¿®æ”¹, creativity éœ€è¦å¤§é‡è¼¸å‡º
    def generate_response(self, user_input, max_tokens=1000):
        """ç”¢ç”Ÿæ¨¡å‹å›æ‡‰"""
        # æ›´æ–°å°è©±æ­·å²
        self.conversation_history.append({"role": "user", "content": user_input})
        
        # å»ºæ§‹prompt
        prompt = self.tokenizer.apply_chat_template(
            self.conversation_history, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        input_length = inputs["input_ids"].shape[1]
        
        # ä½¿ç”¨steeringç”¢ç”Ÿå›æ‡‰
        with ActivationSteerer(
            self.model, 
            self.persona_vectors[self.layer_idx], 
            coeff=self.steering_coef, 
            layer_idx=self.layer_idx-1,
            positions="response"
        ):
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id
                )
        
        # è§£ç¢¼å›æ‡‰
        response = self.tokenizer.decode(
            outputs[0][input_length:], 
            skip_special_tokens=True
        ).strip()
        
        # æ›´æ–°å°è©±æ­·å²
        self.conversation_history.append({"role": "assistant", "content": response})
        
        # é™åˆ¶æ­·å²é•·åº¦
        if len(self.conversation_history) > 10:
            self.conversation_history = self.conversation_history[-10:]
        
        return response
    
    def chat(self):
        """é–‹å§‹äº’å‹•å°è©±"""
        print("âœ… èŠå¤©æ©Ÿå™¨äººæº–å‚™å°±ç·’ï¼")
        print("ğŸ’¬ æŒ‡ä»¤èªªæ˜:")
        print("  - ç›´æ¥è¼¸å…¥æ–‡å­—é–‹å§‹å°è©±")
        print("  - '/layer X' èª¿æ•´ç›®æ¨™å±¤æ•¸")
        print("  - '/coef X' èª¿æ•´steeringå¼·åº¦") 
        print("  - '/reset' æ¸…é™¤å°è©±æ­·å²")
        print("  - '/quit' çµæŸå°è©±")
        print("=" * 50)
        
        while True:
            try:
                user_input = input(f"\nğŸ§‘ You: ").strip()
                
                if not user_input:
                    continue
                
                # è™•ç†æŒ‡ä»¤
                if user_input.startswith('/'):
                    self._handle_command(user_input)
                    continue
                
                if user_input.lower() in ['quit', 'exit']:
                    break
                
                # ç”¢ç”Ÿå›æ‡‰
                print("ğŸ¤– Model: ", end="", flush=True)
                response = self.generate_response(user_input)
                print(response)
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å°è©±å·²ä¸­æ–·ï¼")
                break
            except Exception as e:
                print(f"\nâŒ éŒ¯èª¤: {e}")
    
    def _handle_command(self, command):
        """è™•ç†ç”¨æˆ¶æŒ‡ä»¤"""
        parts = command.split()
        cmd = parts[0].lower()
        
        if cmd == '/layer' and len(parts) == 2:
            try:
                layer = int(parts[1])
                self.set_steering_params(layer_idx=layer)
            except ValueError:
                print("âŒ å±¤æ•¸å¿…é ˆæ˜¯æ•´æ•¸")
        
        elif cmd == '/coef' and len(parts) == 2:
            try:
                coef = float(parts[1])
                self.set_steering_params(coef=coef)
            except ValueError:
                print("âŒ ä¿‚æ•¸å¿…é ˆæ˜¯æ•¸å­—")
        
        elif cmd == '/reset':
            self.conversation_history = []
            print("ğŸ”„ å°è©±æ­·å²å·²æ¸…é™¤")
        
        elif cmd == '/quit':
            print("ğŸ‘‹ å†è¦‹ï¼")
            exit()
        
        else:
            print("âŒ æœªçŸ¥æŒ‡ä»¤")

def main():
    parser = argparse.ArgumentParser(description="Persona Chatbot")
    parser.add_argument("--model", default="Qwen/Qwen2.5-7B-Instruct", help="æ¨¡å‹åç¨±")
    parser.add_argument("--vector_path", required=True, help="personaå‘é‡è·¯å¾‘")
    parser.add_argument("--layer", type=int, default=20, help="ç›®æ¨™å±¤æ•¸")
    parser.add_argument("--coef", type=float, default=2.0, help="steeringä¿‚æ•¸")
    
    args = parser.parse_args()
    
    chatbot = PersonaChatbot(
        model_name=args.model,
        vector_path=args.vector_path,
        layer_idx=args.layer,
        steering_coef=args.coef
    )
    
    chatbot.chat()

if __name__ == "__main__":
    main()