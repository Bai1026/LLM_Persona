#!/usr/bin/env python3
"""
æ¸¬è©¦ä¿®è£œéçš„ eval_persona èˆ‡ Gemma-3 æ•´åˆ
"""

# é¦–å…ˆå°å…¥ä¿®è£œç¨‹å¼
from gemma3_patch import patch_gemma3_sampling

# å•Ÿç”¨ Gemma-3 æ”¯æ´
print("ğŸ”§ å•Ÿç”¨ Gemma-3 æ”¯æ´...")
original_sample = patch_gemma3_sampling()

# ç¾åœ¨æ¸¬è©¦ eval_persona
import subprocess
import os
import pandas as pd

def test_patched_eval_persona():
    """æ¸¬è©¦ä¿®è£œéçš„ eval_persona"""
    
    print("ğŸ§ª æ¸¬è©¦ä¿®è£œéçš„ eval_persona...")
    
    # å»ºç«‹æ¸¬è©¦è¼¸å‡ºè·¯å¾‘
    output_path = "test_gemma3_patched_output.csv"
    
    # åˆªé™¤èˆŠæª”æ¡ˆ
    if os.path.exists(output_path):
        os.remove(output_path)
    
    # å»ºç«‹æŒ‡ä»¤
    cmd = [
        "python", "-c", """
# å°å…¥ä¿®è£œç¨‹å¼
import sys
sys.path.append('.')
from gemma3_patch import patch_gemma3_sampling
patch_gemma3_sampling()

# ç¾åœ¨åŸ·è¡Œ eval_persona
import subprocess
result = subprocess.run([
    'python', '-m', 'eval.eval_persona',
    '--model', 'google/gemma-3-4b-it',
    '--trait', 'creative_professional', 
    '--output_path', 'test_gemma3_patched_output.csv',
    '--persona_instruction_type', 'pos',
    '--assistant_name', 'creative_professional',
    '--judge_model', 'gpt-4o-mini',
    '--version', 'extract'
])
"""
    ]
    
    print(f"ğŸš€ åŸ·è¡Œä¿®è£œæ¸¬è©¦...")
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸
    env = os.environ.copy()
    env["CUDA_VISIBLE_DEVICES"] = "0"
    env["PYTHONPATH"] = "/workspace/LLM_Persona/persona_vectors"
    
    try:
        result = subprocess.run(cmd, env=env, check=True, capture_output=True, text=True)
        print("âœ… ä¿®è£œæ¸¬è©¦åŸ·è¡Œå®Œæˆ")
        
        if result.stdout:
            print("æ¨™æº–è¼¸å‡º:", result.stdout[-500:])  # é¡¯ç¤ºæœ€å¾Œ 500 å­—å…ƒ
        
        # æª¢æŸ¥çµæœ
        if os.path.exists(output_path):
            print(f"ğŸ“Š æª¢æŸ¥çµæœæª”æ¡ˆ: {output_path}")
            df = pd.read_csv(output_path)
            
            if 'answer' in df.columns:
                non_empty = df[df['answer'].notna() & (df['answer'].str.strip() != '')]
                print(f"   éç©ºå›æ‡‰æ¯”ä¾‹: {len(non_empty)}/{len(df)} ({len(non_empty)/len(df)*100:.1f}%)")
                
                if len(non_empty) > 0:
                    print("   å‰ 3 å€‹å›æ‡‰ç¯„ä¾‹:")
                    for i, row in non_empty.head(3).iterrows():
                        answer = str(row['answer'])[:100] + "..." if len(str(row['answer'])) > 100 else str(row['answer'])
                        print(f"     {i+1}: {answer}")
            
            if 'creative_professional' in df.columns:
                scores = df['creative_professional'].dropna()
                if len(scores) > 0:
                    print(f"   å¹³å‡å‰µæ„åˆ†æ•¸: {scores.mean():.4f}")
        
        return True
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        if e.stdout:
            print("æ¨™æº–è¼¸å‡º:", e.stdout)
        if e.stderr:
            print("éŒ¯èª¤è¼¸å‡º:", e.stderr)
        return False

def simple_direct_test():
    """ç›´æ¥æ¸¬è©¦ä¿®è£œåŠŸèƒ½"""
    
    print("\nğŸ”¬ ç›´æ¥æ¸¬è©¦ä¿®è£œåŠŸèƒ½...")
    
    try:
        # æ¨¡æ“¬ eval_persona çš„è¼‰å…¥éç¨‹
        from eval.model_utils import load_vllm_model
        
        print("è¼‰å…¥ Gemma-3 æ¨¡å‹...")
        llm, tokenizer, lora_path = load_vllm_model("google/gemma-3-4b-it")
        
        print(f"æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œåˆ†è©å™¨: {type(tokenizer).__name__}")
        
        # æ¸¬è©¦èŠå¤©æ¨¡æ¿
        test_messages = [
            {"role": "user", "content": "What is creativity?"}
        ]
        
        try:
            prompt = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)
            print(f"èŠå¤©æ¨¡æ¿æ¸¬è©¦æˆåŠŸ:")
            print(f"  æç¤º: {prompt}")
        except Exception as e:
            print(f"èŠå¤©æ¨¡æ¿æ¸¬è©¦å¤±æ•—: {e}")
        
        # æ¸¬è©¦ä¿®è£œéçš„ sample å‡½å¼
        from eval.eval_persona import sample
        
        print("æ¸¬è©¦ä¿®è£œéçš„ sample å‡½å¼...")
        conversations = [test_messages]
        
        try:
            texts, answers = sample(
                model=llm, 
                tokenizer=tokenizer, 
                conversations=conversations,
                max_tokens=50,
                temperature=0.7,
                min_tokens=5
            )
            
            print(f"ç”¢ç”Ÿæ¸¬è©¦å®Œæˆ:")
            print(f"  è¼¸å…¥: {texts[0][:100]}...")
            print(f"  è¼¸å‡º: '{answers[0]}'")
            
            if answers[0].strip():
                print("âœ… ä¿®è£œæˆåŠŸï¼æ¨¡å‹æœ‰å›æ‡‰")
                return True
            else:
                print("âš ï¸ å›æ‡‰ä»ç„¶æ˜¯ç©ºçš„")
                return False
                
        except Exception as e:
            print(f"sample å‡½å¼æ¸¬è©¦å¤±æ•—: {e}")
            return False
            
    except Exception as e:
        print(f"ç›´æ¥æ¸¬è©¦å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹ Gemma-3 ä¿®è£œæ¸¬è©¦...")
    
    # å…ˆé€²è¡Œç›´æ¥æ¸¬è©¦
    direct_success = simple_direct_test()
    
    if direct_success:
        print("\n" + "="*60)
        print("âœ… ç›´æ¥æ¸¬è©¦æˆåŠŸï¼Œé€²è¡Œå®Œæ•´æ¸¬è©¦...")
        test_patched_eval_persona()
    else:
        print("\nâŒ ç›´æ¥æ¸¬è©¦å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿æ•´ä¿®è£œç¨‹å¼")
