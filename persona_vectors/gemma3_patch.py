"""
ç‚º Gemma-3 æ¨¡å‹ä¿®æ­£ eval_persona.py ä¸­çš„ç”¢ç”Ÿåƒæ•¸
"""
import torch
from vllm import SamplingParams

def get_gemma3_sampling_params(tokenizer, temperature=1, top_p=1, max_tokens=1000, min_tokens=10):
    """ç‚º Gemma-3 æ¨¡å‹ç²å–æœ€ä½³åŒ–çš„ SamplingParams"""
    
    # Gemma-3 çš„ç‰¹æ®Š stop tokens
    stop_tokens = []
    
    # åŠ å…¥æ¨™æº–çš„çµæŸ token
    if tokenizer.eos_token:
        stop_tokens.append(tokenizer.eos_token)
    
    # åŠ å…¥ Gemma ç‰¹æœ‰çš„å°è©±çµæŸ token
    gemma_stop_tokens = ["<end_of_turn>", "</s>", "<eos>"]
    for token in gemma_stop_tokens:
        if token not in stop_tokens:
            stop_tokens.append(token)
    
    return SamplingParams(
        temperature=temperature,
        top_p=top_p,
        max_tokens=max_tokens,
        skip_special_tokens=True,
        stop=stop_tokens,
        min_tokens=min_tokens,  # å¢åŠ æœ€å° token æ•¸é‡
        repetition_penalty=1.1,  # åŠ å…¥é‡è¤‡æ‡²ç½°
        length_penalty=1.0       # åŠ å…¥é•·åº¦æ‡²ç½°
    )

def is_gemma_model(model_name: str) -> bool:
    """åˆ¤æ–·æ˜¯å¦ç‚º Gemma æ¨¡å‹"""
    return "gemma" in model_name.lower()

def patch_gemma3_sampling():
    """ç‚º Gemma-3 ä¿®è£œ eval_persona.py ä¸­çš„æ¡æ¨£å‡½å¼"""
    
    import eval.eval_persona as eval_persona_module
    
    # å‚™ä»½åŸå§‹å‡½å¼
    original_sample = eval_persona_module.sample
    
    def sample_with_gemma3_support(model, tokenizer, conversations, top_p=1, max_tokens=1000, temperature=1, min_tokens=1, lora_path=None):
        """æ”¯æ´ Gemma-3 çš„æ¡æ¨£å‡½å¼"""
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Gemma æ¨¡å‹
        model_name = getattr(model, 'model_config', {}).get('model', '')
        if not model_name:
            # å˜—è©¦å¾ tokenizer ç²å–æ¨¡å‹åç¨±
            model_name = getattr(tokenizer, 'name_or_path', '')
        
        if is_gemma_model(model_name):
            print(f"ğŸ¯ åµæ¸¬åˆ° Gemma æ¨¡å‹ï¼Œä½¿ç”¨æœ€ä½³åŒ–åƒæ•¸: {model_name}")
            
            # ä½¿ç”¨ Gemma-3 æœ€ä½³åŒ–çš„åƒæ•¸
            sampling_params = get_gemma3_sampling_params(
                tokenizer=tokenizer,
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                min_tokens=max(min_tokens, 10)  # è‡³å°‘ 10 å€‹ token
            )
        else:
            # ä½¿ç”¨åŸå§‹åƒæ•¸
            sampling_params = SamplingParams(
                temperature=temperature,
                top_p=top_p,
                max_tokens=max_tokens,
                skip_special_tokens=True,
                stop=[tokenizer.eos_token] if tokenizer.eos_token else [],
                min_tokens=min_tokens
            )
        
        # æº–å‚™æç¤ºæ–‡å­—
        texts = []
        for i, messages in enumerate(conversations):
            try:
                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                texts.append(text)
            except Exception as e:
                print(f"âš ï¸ èŠå¤©æ¨¡æ¿è™•ç†å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®æ ¼å¼: {e}")
                # å›é€€åˆ°ç°¡å–®æ ¼å¼
                if isinstance(messages, list) and len(messages) > 0:
                    if isinstance(messages[-1], dict) and 'content' in messages[-1]:
                        texts.append(messages[-1]['content'])
                    else:
                        texts.append(str(messages[-1]))
                else:
                    texts.append(str(messages))
        
        generate_kwargs = {
            "sampling_params": sampling_params,
            "use_tqdm": True
        }
        
        try:
            if lora_path:
                from vllm.lora.request import LoRARequest
                completions = model.generate(texts, **generate_kwargs, lora_request=LoRARequest("default", 1, lora_path=lora_path))
            else:
                completions = model.generate(texts, **generate_kwargs)
            
            answers = [completion.outputs[0].text for completion in completions]
            
            # é™¤éŒ¯ï¼šæª¢æŸ¥æ˜¯å¦æœ‰ç©ºå›æ‡‰
            empty_count = sum(1 for answer in answers if not answer.strip())
            if empty_count > 0:
                print(f"âš ï¸ åµæ¸¬åˆ° {empty_count}/{len(answers)} å€‹ç©ºå›æ‡‰")
                # é¡¯ç¤ºå‰å¹¾å€‹ç¯„ä¾‹
                for i, (text, answer) in enumerate(zip(texts[:3], answers[:3])):
                    print(f"ç¯„ä¾‹ {i+1}:")
                    print(f"  è¼¸å…¥: {text[:100]}...")
                    print(f"  è¼¸å‡º: '{answer}'")
            
            return texts, answers
            
        except Exception as e:
            print(f"âŒ ç”¢ç”Ÿå¤±æ•—: {e}")
            # å›é€€ç­–ç•¥ï¼šè¿”å›ç©ºç­”æ¡ˆä½†ä¿æŒæ ¼å¼ä¸€è‡´
            return texts, ["" for _ in texts]
    
    # æ›¿æ›åŸå§‹å‡½å¼
    eval_persona_module.sample = sample_with_gemma3_support
    print("âœ… Gemma-3 æ¡æ¨£æ”¯æ´å·²å•Ÿç”¨")
    
    return original_sample

if __name__ == "__main__":
    print("ğŸ”§ ä¿®è£œ eval_persona.py ä»¥æ”¯æ´ Gemma-3...")
    patch_gemma3_sampling()
    print("âœ… ä¿®è£œå®Œæˆ")
