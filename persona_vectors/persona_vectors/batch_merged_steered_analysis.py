import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from rich import print
import csv
import os
from datetime import datetime

# --- è¼”åŠ©å‡½æ•¸ ---
def a_proj_b(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """è¨ˆç®—å‘é‡ a åœ¨å‘é‡ b ä¸Šçš„ç´”é‡æŠ•å½±ã€‚"""
    if a.dtype != b.dtype:
        a = a.to(b.dtype)
    
    b_norm = b.norm()
    return torch.tensor(0.0, dtype=b.dtype, device=b.device) if b_norm == 0 else torch.dot(a, b) / b_norm

# --- è¨­å®šåƒæ•¸ ---
MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
VECTOR_A_PATH = "Llama-3.1-8B-Instruct/multi_role/creative_professional_response_avg_diff.pt"
VECTOR_B_PATH = "Llama-3.1-8B-Instruct/multi_role/environmentalist_response_avg_diff.pt"

STEERING_LAYER = 20
STEERING_COEFFICIENT = 2.0
ANALYSIS_LAYERS = list(range(20, 33))  # 20-32

# --- 10 å€‹æ¸¬è©¦é¡Œç›® ---
AUT_LIST = [
    "What are some creative use for Fork? The goal is to come up with creative ideas, which are ideas that strike people as clever, unusual, interesting, uncommon, humorous, innovative, or different. Present a list of 5 creative and diverse uses for Fork.",
    "What are some creative use for Jar? The goal is to come up with creative ideas, which are ideas that strike people as clever, unusual, interesting, uncommon, humorous, innovative, or different. Present a list of 5 creative and diverse uses for Jar."
]

INS_LIST = [
    "Name all the round things you can think of.",
    "Name all the things you can think of that will make a noise.",
    "Name all the things you can think of that have a screen."
]

SIMI_LIST = [
    "Tell me all the ways in which a kite and a balloon are alike.",
    "Tell me all the ways in which a pencil and a pen are alike.",
    "Tell me all the ways in which a chair and a couch are alike."
]

SCI_LIST = [
    "If you can take a spaceship to travel in outer space and go to a planet, what scientific questions do you want to research? For example, are there any living things on the planet?",
    "Please think up as many possible improvements as you can to a regular bicycle, making it more interesting, more useful and more beautiful. For example, make the tires reflective, so they can be seen in the dark."
]

NEUTRAL_LIST = [
    "Urban Planning (2050 City Block Masterplan): Design a masterplan for a new city block to be built in 2050. Describe core principles, layout, mobility, public space, services, and governance constraints.",
    "Product Launch (Micro-Teleportation for Small Objects): Outline a public launch plan for a micro-teleportation technology for small items. Include positioning, safety/regulation, go-to-market, operations, and risk.",
    "Social Issue (Countering Misinformation): Propose a multi-pronged plan to reduce misinformation on social platforms: policy, product, incentives, literacy, measurement.",
    "Corporate Strategy (Legacy Manufacturer vs. AI Disruption): Design a transformation strategy for a legacy manufacturer facing AI disruption: portfolio, org, tech stack, talent, risk, timeline.",
    "Healthcare Innovation (Reimagine the Hospital): Redesign the future hospital experience for patients, families, and staff. Address flows, safety, data, wellbeing, equity, and feasibility.",
    "Education Reform (Ideal High-School Curriculum): Propose a 4-year curriculum: core subjects, skills, experiential learning, assessment, inclusion, and teacher enablement.",
    "Disaster Response (Early Recovery Plan for a Metro Area): Draft an initial 30â€“60 day recovery plan after a major natural disaster: assessment, triage, logistics, comms, governance, equity.",
    "Space Exploration (Next 50 Years Priority): State and justify the top priority for human space exploration in the next 50 years. Define milestones, risks, ethics, and spillovers.",
    "Sustainable Fashion (Net-Zero Brand Model): Propose a business model for a fully sustainable fashion brand: materials, supply chain, circularity, economics, verification, storytelling.",
    "Global Challenge (Food Waste Reduction): Design a multi-layer plan to reduce global food waste across production, retail, and households: incentives, infra, tech, policy, culture."
]

# PROMPT_LIST = AUT_LIST + INS_LIST + SIMI_LIST + SCI_LIST
PROMPT_LIST = NEUTRAL_LIST

def process_single_prompt_complete_analysis(prompt, prompt_idx, model, tokenizer, steering_layer, 
                                         analysis_layers, vector_a_steer, vector_b_steer, 
                                         vectors_a_all, vectors_b_all, steering_coefficient, device):
    """è™•ç†å–®å€‹é¡Œç›®çš„å®Œæ•´åˆ†æï¼šä¸€æ¬¡åŸºæº– + ä¸€æ¬¡å¼•å°ï¼Œåˆ†ææ‰€æœ‰å±¤"""
    
    print(f"\nè™•ç†é¡Œç›® {prompt_idx+1}/{len(PROMPT_LIST)}")
    print(f"é¡Œç›®: {prompt[:80]}...")
    
    # æº–å‚™è¼¸å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    prompt_len = inputs.input_ids.shape[1]
    
    print("  ğŸ”„ åŸ·è¡ŒåŸºæº–æ¸¬è©¦ï¼ˆä¸åŠ å¼•å°ï¼‰...")
    # --- 1. åŸºæº–æ¸¬è©¦ï¼šè’é›†æ‰€æœ‰åˆ†æå±¤çš„åŸºæº– activation ---
    with torch.no_grad():
        outputs_baseline = model.generate(
            **inputs, 
            max_new_tokens=512, 
            output_hidden_states=True, 
            return_dict_in_generate=True
        )

    # æå–æ‰€æœ‰åˆ†æå±¤çš„åŸºæº– activation
    baseline_activations = {}
    for analysis_layer in analysis_layers:
        generated_hidden_states = []
        for token_idx, token_hidden_states in enumerate(outputs_baseline.hidden_states):
            if token_idx >= prompt_len:
                layer_activation = token_hidden_states[analysis_layer]
                generated_hidden_states.append(layer_activation[:, -1, :])
        
        if generated_hidden_states:
            baseline_activations[analysis_layer] = torch.stack(generated_hidden_states).mean(dim=0).squeeze()
        else:
            baseline_activations[analysis_layer] = outputs_baseline.hidden_states[0][analysis_layer][:, -1, :].squeeze()

    print("  ğŸ¯ åŸ·è¡Œå¼•å°æ¸¬è©¦ï¼ˆç¬¬20å±¤åŠ å¼•å°ï¼‰...")
    # --- 2. å¼•å°æ¸¬è©¦ï¼šåœ¨ç¬¬20å±¤åŠ å¼•å°ï¼Œè’é›†æ‰€æœ‰åˆ†æå±¤çš„å¼•å°å¾Œ activation ---
    steering_vec = (vector_a_steer + vector_b_steer) * steering_coefficient

    def steering_hook(module, args, kwargs, output):
        modified_output = output[0] + steering_vec
        return (modified_output,) + output[1:]

    hook_handle = model.model.layers[steering_layer].register_forward_hook(steering_hook, with_kwargs=True)
    
    try:
        with torch.no_grad():
            outputs_steered = model.generate(
                **inputs, 
                max_new_tokens=512, 
                output_hidden_states=True, 
                return_dict_in_generate=True
            )
    finally:
        hook_handle.remove()

    # æå–æ‰€æœ‰åˆ†æå±¤çš„å¼•å°å¾Œ activation
    steered_activations = {}
    for analysis_layer in analysis_layers:
        generated_hidden_states = []
        for token_idx, token_hidden_states in enumerate(outputs_steered.hidden_states):
            if token_idx >= prompt_len:
                layer_activation = token_hidden_states[analysis_layer]
                generated_hidden_states.append(layer_activation[:, -1, :])
        
        if generated_hidden_states:
            steered_activations[analysis_layer] = torch.stack(generated_hidden_states).mean(dim=0).squeeze()
        else:
            steered_activations[analysis_layer] = outputs_steered.hidden_states[0][analysis_layer][:, -1, :].squeeze()

    print("  ğŸ“Š è¨ˆç®—å„å±¤æŠ•å½±åˆ†æ...")
    # --- 3. å°æ¯å€‹åˆ†æå±¤é€²è¡ŒæŠ•å½±åˆ†æ ---
    results = []
    baseline_text = tokenizer.decode(outputs_baseline.sequences[0][prompt_len:])
    steered_text = tokenizer.decode(outputs_steered.sequences[0][prompt_len:])
    
    for analysis_layer in analysis_layers:
        # å–å¾—è©²å±¤çš„æŠ•å½±å‘é‡
        vector_a_proj = vectors_a_all[analysis_layer]
        vector_b_proj = vectors_b_all[analysis_layer]
        
        # å–å¾—è©²å±¤çš„ activation
        activation_baseline = baseline_activations[analysis_layer]
        activation_steered = steered_activations[analysis_layer]
        
        # 5a. çµ•å°æŠ•å½±åˆ†æ
        proj_baseline_on_a = a_proj_b(activation_baseline, vector_a_proj)
        proj_baseline_on_b = a_proj_b(activation_baseline, vector_b_proj)
        proj_steered_on_a = a_proj_b(activation_steered, vector_a_proj)
        proj_steered_on_b = a_proj_b(activation_steered, vector_b_proj)

        # 5b. å·®å€¼æŠ•å½±åˆ†æ
        activation_delta = activation_steered - activation_baseline
        proj_delta_on_a = a_proj_b(activation_delta, vector_a_proj)
        proj_delta_on_b = a_proj_b(activation_delta, vector_b_proj)
        
        result = {
            'prompt_idx': prompt_idx,
            'prompt': prompt,
            'steering_layer': steering_layer,
            'analysis_layer': analysis_layer,
            'steering_coefficient': steering_coefficient,
            'proj_baseline_on_a': proj_baseline_on_a.item(),
            'proj_baseline_on_b': proj_baseline_on_b.item(),
            'proj_steered_on_a': proj_steered_on_a.item(),
            'proj_steered_on_b': proj_steered_on_b.item(),
            'proj_delta_on_a': proj_delta_on_a.item(),
            'proj_delta_on_b': proj_delta_on_b.item(),
            'baseline_text': baseline_text,
            'steered_text': steered_text
        }
        results.append(result)
        
        print(f"    å±¤ {analysis_layer}: AåŸºæº–={proj_baseline_on_a:.3f}, BåŸºæº–={proj_baseline_on_b:.3f}")
    
    return results

def main():
    print("="*50)
    print("æ‰¹æ¬¡å¼•å°å‘é‡åˆ†æå¯¦é©—")
    print("="*50)
    print(f"æ¨¡å‹: {MODEL_NAME}")
    print(f"å¼•å°å±¤: {STEERING_LAYER}")
    print(f"åˆ†æå±¤ç¯„åœ: {ANALYSIS_LAYERS[0]}-{ANALYSIS_LAYERS[-1]}")
    print(f"å¼•å°ä¿‚æ•¸: {STEERING_COEFFICIENT}")
    print(f"é¡Œç›®ç¸½æ•¸: {len(PROMPT_LIST)}")
    print("="*50)

    # åˆå§‹åŒ–
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16).to(device)
    model.eval()

    model_dtype = torch.bfloat16
    
    # è¼‰å…¥å¼•å°å‘é‡ï¼ˆå›ºå®šåœ¨ç¬¬20å±¤ï¼‰
    vectors_a_all = torch.load(VECTOR_A_PATH, map_location=device)
    vectors_b_all = torch.load(VECTOR_B_PATH, map_location=device)
    
    vector_a_steer = vectors_a_all[STEERING_LAYER].to(dtype=model_dtype)
    vector_b_steer = vectors_b_all[STEERING_LAYER].to(dtype=model_dtype)
    
    # æ­£è¦åŒ–å¼•å°å‘é‡
    vector_a_steer /= vector_a_steer.norm()
    vector_b_steer /= vector_b_steer.norm()
    
    # æº–å‚™æ‰€æœ‰åˆ†æå±¤çš„æŠ•å½±å‘é‡ä¸¦æ­£è¦åŒ–
    for layer in ANALYSIS_LAYERS:
        vectors_a_all[layer] = vectors_a_all[layer].to(dtype=model_dtype)
        vectors_b_all[layer] = vectors_b_all[layer].to(dtype=model_dtype)
        vectors_a_all[layer] /= vectors_a_all[layer].norm()
        vectors_b_all[layer] /= vectors_b_all[layer].norm()

    # æº–å‚™çµæœå„²å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"batch_steering_results_{timestamp}.csv"
    detailed_results_file = f"detailed_results_{timestamp}.txt"
    
    all_results = []
    
    # åŸ·è¡Œæ‰¹æ¬¡å¯¦é©—
    total_experiments = len(PROMPT_LIST)  # æ¯å€‹é¡Œç›®åªéœ€è¦è™•ç†ä¸€æ¬¡
    
    for prompt_idx, prompt in enumerate(PROMPT_LIST):
        print(f"\n{'='*60}")
        print(f"è™•ç†é¡Œç›® {prompt_idx+1}/{len(PROMPT_LIST)}")
        print(f"é¡Œç›®: {prompt}")
        print(f"{'='*60}")
        
        # åŸ·è¡Œå–®å€‹é¡Œç›®çš„å®Œæ•´åˆ†æï¼ˆä¸€æ¬¡åŸºæº– + ä¸€æ¬¡å¼•å°ï¼Œåˆ†ææ‰€æœ‰å±¤ï¼‰
        prompt_results = process_single_prompt_complete_analysis(
            prompt, prompt_idx, model, tokenizer, 
            STEERING_LAYER, ANALYSIS_LAYERS,
            vector_a_steer, vector_b_steer, 
            vectors_a_all, vectors_b_all,
            STEERING_COEFFICIENT, device
        )
        
        all_results.extend(prompt_results)
        
        # å³æ™‚é¡¯ç¤ºçµæœæ‘˜è¦
        print(f"\n  âœ… å®Œæˆé¡Œç›® {prompt_idx+1}ï¼Œç”¢ç”Ÿ {len(prompt_results)} ç­†åˆ†æçµæœ")

    # å„²å­˜çµæœåˆ° CSV
    print(f"\nå„²å­˜çµæœåˆ° {results_file}")
    with open(results_file, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'prompt_idx', 'analysis_layer', 'steering_layer', 'steering_coefficient',
            'proj_baseline_on_a', 'proj_baseline_on_b', 
            'proj_steered_on_a', 'proj_steered_on_b',
            'proj_delta_on_a', 'proj_delta_on_b'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for result in all_results:
            row = {k: v for k, v in result.items() if k in fieldnames}
            writer.writerow(row)

    # å„²å­˜è©³ç´°çµæœï¼ˆåŒ…å«æ–‡æœ¬ï¼‰
    print(f"å„²å­˜è©³ç´°çµæœåˆ° {detailed_results_file}")
    with open(detailed_results_file, 'w', encoding='utf-8') as f:
        f.write("æ‰¹æ¬¡å¼•å°å‘é‡åˆ†æè©³ç´°çµæœ\n")
        f.write("="*60 + "\n\n")
        
        for result in all_results:
            f.write(f"é¡Œç›® {result['prompt_idx']+1}, åˆ†æå±¤ {result['analysis_layer']}\n")
            f.write(f"Prompt: {result['prompt']}\n")
            f.write(f"å¼•å°å±¤: {result['steering_layer']}, åˆ†æå±¤: {result['analysis_layer']}, å¼•å°ä¿‚æ•¸: {result['steering_coefficient']}\n\n")
            
            f.write("--- 5a. çµ•å°æŠ•å½±åˆ†æ (æ¸¬é‡æœ€çµ‚ã€æ€ç¶­ç‹€æ…‹ã€) ---\n")
            f.write(f"åŸºæº–æŠ•å½±: A={result['proj_baseline_on_a']:.4f}, B={result['proj_baseline_on_b']:.4f}\n")
            f.write(f"å¼•å°å¾ŒæŠ•å½±: A={result['proj_steered_on_a']:.4f}, B={result['proj_steered_on_b']:.4f}\n\n")
            
            f.write("--- 5b. å·®å€¼æŠ•å½±åˆ†æ (æ¸¬é‡å¼•å°å‘é‡çš„ã€å› æœæ•ˆæ‡‰ã€) ---\n")
            f.write(f"æ¿€æ´»è®ŠåŒ–é‡åœ¨ A ä¸Šçš„æŠ•å½±: {result['proj_delta_on_a']:.4f}\n")
            f.write(f"æ¿€æ´»è®ŠåŒ–é‡åœ¨ B ä¸Šçš„æŠ•å½±: {result['proj_delta_on_b']:.4f}\n\n")
            
            f.write("--- ç”Ÿæˆæ–‡æœ¬å°æ¯” ---\n")
            f.write(f"ã€åŸºæº–ã€‘: {result['baseline_text']}\n")
            f.write(f"ã€å¼•å°å¾Œã€‘: {result['steered_text']}\n")
            f.write("\n" + "="*60 + "\n\n")

    print(f"\nâœ… å¯¦é©—å®Œæˆï¼")
    print(f"ğŸ“Š CSV çµæœ: {results_file}")
    print(f"ğŸ“ è©³ç´°çµæœ: {detailed_results_file}")
    print(f"ğŸ”¢ ç¸½å¯¦é©—æ¬¡æ•¸: {len(all_results)}")
    print(f"ğŸ“‹ é¡Œç›®æ•¸é‡: {len(PROMPT_LIST)}")
    print(f"ğŸ¯ åˆ†æå±¤æ•¸é‡: {len(ANALYSIS_LAYERS)}")

if __name__ == "__main__":
    main()