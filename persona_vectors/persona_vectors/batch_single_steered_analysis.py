import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from rich import print
import csv
import os
from datetime import datetime

# --- è¼”åŠ©å‡½å¼ ---
def a_proj_b(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """è¨ˆç®—å‘é‡ a åœ¨å‘é‡ b ä¸Šçš„ç´”é‡æŠ•å½±ã€‚"""
    if a.dtype != b.dtype:
        a = a.to(b.dtype)
    
    b_norm = b.norm()
    return torch.tensor(0.0, dtype=b.dtype, device=b.device) if b_norm == 0 else torch.dot(a, b) / b_norm

# --- è¨­å®šåƒæ•¸ ---
MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
VECTOR_A_PATH = "Llama-3.1-8B-Instruct/multi_role/creative_professional_response_avg_diff.pt"

STEERING_LAYER = 20  # å¼•å°å±¤
STEERING_COEFFICIENT = 2.0
ANALYSIS_LAYERS = list(range(20, 33))  # 20-32 è§€å¯Ÿå±¤

# --- æ¸¬è©¦é¡Œç›® ---
PROMPT_LIST = [
    "What are some creative use for Fork? The goal is to come up with creative ideas, which are ideas that strike people as clever, unusual, interesting, uncommon, humorous, innovative, or different. Present a list of 5 creative and diverse uses for Fork.",
    "Urban Planning (2050 City Block Masterplan): Design a masterplan for a new city block to be built in 2050. Describe core principles, layout, mobility, public space, services, and governance constraints.",
    "Product Launch (Micro-Teleportation for Small Objects): Outline a public launch plan for a micro-teleportation technology for small items. Include positioning, safety/regulation, go-to-market, operations, and risk."
]

def process_single_vector_steered_analysis(prompt, prompt_idx, model, tokenizer, steering_layer, 
                                         analysis_layers, steering_vector, 
                                         projection_vectors_all, steering_coefficient, device):
    """è™•ç†å–®å€‹é¡Œç›®çš„å–®ä¸€å‘é‡å¼•å°åˆ†æ"""
    
    print(f"\nè™•ç†é¡Œç›® {prompt_idx+1}/{len(PROMPT_LIST)}")
    print(f"é¡Œç›®: {prompt[:80]}...")
    
    # æº–å‚™è¼¸å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    prompt_len = inputs.input_ids.shape[1]
    
    print("  ğŸ”„ åŸ·è¡ŒåŸºæº–æ¸¬è©¦ï¼ˆä¸åŠ å¼•å°ï¼‰...")
    # --- 1. åŸºæº–æ¸¬è©¦ ---
    with torch.no_grad():
        outputs_baseline = model.generate(
            **inputs, 
            max_new_tokens=512, 
            output_hidden_states=True, 
            return_dict_in_generate=True
        )

    # æå–æ‰€æœ‰åˆ†æå±¤çš„åŸºæº– activation
    baseline_activations = {}
    for analysis_layer in analysis_layers:
        generated_hidden_states = []
        for token_idx, token_hidden_states in enumerate(outputs_baseline.hidden_states):
            if token_idx >= prompt_len:
                layer_activation = token_hidden_states[analysis_layer]
                generated_hidden_states.append(layer_activation[:, -1, :])
        
        if generated_hidden_states:
            baseline_activations[analysis_layer] = torch.stack(generated_hidden_states).mean(dim=0).squeeze()
        else:
            baseline_activations[analysis_layer] = outputs_baseline.hidden_states[0][analysis_layer][:, -1, :].squeeze()

    print(f"  ğŸ¯ åŸ·è¡Œå–®ä¸€å‘é‡å¼•å°æ¸¬è©¦ï¼ˆç¬¬{steering_layer}å±¤åŠ å¼•å°ï¼‰...")
    # --- 2. å–®ä¸€å‘é‡å¼•å°æ¸¬è©¦ ---
    steering_vec = steering_vector * steering_coefficient

    def steering_hook(module, args, kwargs, output):
        modified_output = output[0] + steering_vec
        return (modified_output,) + output[1:]

    hook_handle = model.model.layers[steering_layer].register_forward_hook(steering_hook, with_kwargs=True)
    
    try:
        with torch.no_grad():
            outputs_steered = model.generate(
                **inputs, 
                max_new_tokens=512, 
                output_hidden_states=True, 
                return_dict_in_generate=True
            )
    finally:
        hook_handle.remove()

    # æå–æ‰€æœ‰åˆ†æå±¤çš„å¼•å°å¾Œ activation
    steered_activations = {}
    for analysis_layer in analysis_layers:
        generated_hidden_states = []
        for token_idx, token_hidden_states in enumerate(outputs_steered.hidden_states):
            if token_idx >= prompt_len:
                layer_activation = token_hidden_states[analysis_layer]
                generated_hidden_states.append(layer_activation[:, -1, :])
        
        if generated_hidden_states:
            steered_activations[analysis_layer] = torch.stack(generated_hidden_states).mean(dim=0).squeeze()
        else:
            steered_activations[analysis_layer] = outputs_steered.hidden_states[0][analysis_layer][:, -1, :].squeeze()

    print("  ğŸ“Š è¨ˆç®—å„å±¤å°å¼•å°å‘é‡çš„æŠ•å½±åˆ†æ...")
    # --- 3. å°æ¯å€‹åˆ†æå±¤é€²è¡ŒæŠ•å½±åˆ†æ ---
    results = []
    baseline_text = tokenizer.decode(outputs_baseline.sequences[0][prompt_len:])
    steered_text = tokenizer.decode(outputs_steered.sequences[0][prompt_len:])
    
    for analysis_layer in analysis_layers:
        # å–å¾—è©²å±¤çš„æŠ•å½±å‘é‡ï¼ˆç”¨åŒä¸€å€‹å¼•å°å‘é‡åšæŠ•å½±ï¼‰
        projection_vector = projection_vectors_all[analysis_layer]
        
        # å–å¾—è©²å±¤çš„ activation
        activation_baseline = baseline_activations[analysis_layer]
        activation_steered = steered_activations[analysis_layer]
        
        # è¨ˆç®—æŠ•å½±
        proj_baseline = a_proj_b(activation_baseline, projection_vector)
        proj_steered = a_proj_b(activation_steered, projection_vector)
        
        # å·®å€¼åˆ†æ
        activation_delta = activation_steered - activation_baseline
        proj_delta = a_proj_b(activation_delta, projection_vector)
        
        result = {
            'prompt_idx': prompt_idx,
            'prompt': prompt,
            'steering_layer': steering_layer,
            'analysis_layer': analysis_layer,
            'steering_coefficient': steering_coefficient,
            'proj_baseline': proj_baseline.item(),
            'proj_steered': proj_steered.item(),
            'proj_delta': proj_delta.item(),
            'baseline_text': baseline_text,
            'steered_text': steered_text
        }
        results.append(result)
        
        print(f"    å±¤ {analysis_layer}: åŸºæº–={proj_baseline:.3f}, å¼•å°å¾Œ={proj_steered:.3f}, è®ŠåŒ–={proj_delta:.3f}")
    
    return results

def main():
    print("="*50)
    print("å–®ä¸€å‘é‡å¼•å°åˆ†æå¯¦é©—")
    print("="*50)
    print(f"æ¨¡å‹: {MODEL_NAME}")
    print(f"å¼•å°å±¤: {STEERING_LAYER}")
    print(f"åˆ†æå±¤ç¯„åœ: {ANALYSIS_LAYERS[0]}-{ANALYSIS_LAYERS[-1]}")
    print(f"å¼•å°ä¿‚æ•¸: {STEERING_COEFFICIENT}")
    print(f"é¡Œç›®ç¸½æ•¸: {len(PROMPT_LIST)}")
    print("="*50)

    # åˆå§‹åŒ–
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ä½¿ç”¨è¨­å‚™: {device}")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.bfloat16).to(device)
    model.eval()

    model_dtype = torch.bfloat16
    
    # è¼‰å…¥å‘é‡
    vectors_all = torch.load(VECTOR_A_PATH, map_location=device)
    
    # æº–å‚™å¼•å°å‘é‡ï¼ˆç¬¬20å±¤ï¼‰
    steering_vector = vectors_all[STEERING_LAYER].to(dtype=model_dtype)
    steering_vector = steering_vector / steering_vector.norm()  # æ­£è¦åŒ–
    
    # æº–å‚™æ‰€æœ‰åˆ†æå±¤çš„æŠ•å½±å‘é‡ä¸¦æ­£è¦åŒ–ï¼ˆç”¨åŒä¸€å€‹å‘é‡ï¼‰
    projection_vectors_all = {}
    for layer in ANALYSIS_LAYERS:
        # projection_vectors_all[layer] = vectors_all[layer].to(dtype=model_dtype)
        projection_vectors_all[layer] = steering_vector 
        projection_vectors_all[layer] = projection_vectors_all[layer] / projection_vectors_all[layer].norm()

    # æº–å‚™çµæœå„²å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"single_vector_steering_results_{timestamp}.csv"
    
    all_results = []
    
    # åŸ·è¡Œæ‰¹æ¬¡å¯¦é©—
    for prompt_idx, prompt in enumerate(PROMPT_LIST):
        print(f"\n{'='*60}")
        print(f"è™•ç†é¡Œç›® {prompt_idx+1}/{len(PROMPT_LIST)}")
        print(f"{'='*60}")
        
        prompt_results = process_single_vector_steered_analysis(
            prompt, prompt_idx, model, tokenizer, 
            STEERING_LAYER, ANALYSIS_LAYERS,
            steering_vector, projection_vectors_all,
            STEERING_COEFFICIENT, device
        )
        
        all_results.extend(prompt_results)
        print(f"\n  âœ… å®Œæˆé¡Œç›® {prompt_idx+1}")

    # å„²å­˜çµæœåˆ° CSV
    print(f"\nå„²å­˜çµæœåˆ° {results_file}")
    with open(results_file, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = [
            'prompt_idx', 'analysis_layer', 'steering_layer', 'steering_coefficient',
            'proj_baseline', 'proj_steered', 'proj_delta'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for result in all_results:
            row = {k: v for k, v in result.items() if k in fieldnames}
            writer.writerow(row)

    # é¡¯ç¤ºçµæœæ‘˜è¦
    print(f"\nâœ… å¯¦é©—å®Œæˆï¼")
    print(f"ğŸ“Š çµæœæª”æ¡ˆ: {results_file}")
    print(f"ğŸ”¢ ç¸½åˆ†ææ•¸é‡: {len(all_results)}")
    
    # é¡¯ç¤ºç¬¬20å±¤çš„æŠ•å½±å€¼ï¼ˆæ‡‰è©²æ¥è¿‘1ï¼‰
    layer_20_results = [r for r in all_results if r['analysis_layer'] == 20]
    if layer_20_results:
        avg_proj_steered_20 = sum(r['proj_steered'] for r in layer_20_results) / len(layer_20_results)
        print(f"ğŸ¯ ç¬¬20å±¤å¹³å‡å¼•å°å¾ŒæŠ•å½±: {avg_proj_steered_20:.4f} (é æœŸæ¥è¿‘1.0)")

if __name__ == "__main__":
    main()