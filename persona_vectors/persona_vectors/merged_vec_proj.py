import torch
import os
import argparse
from pathlib import Path

def a_proj_b(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """
    è¨ˆç®—å‘é‡ a åœ¨å‘é‡ b ä¸Šçš„ç´”é‡æŠ•å½± (scalar projection)ã€‚
    çµæœæ˜¯ä¸€å€‹ç´”é‡ï¼Œä»£è¡¨ a åœ¨ b æ–¹å‘ä¸Šçš„æœ‰å‘é•·åº¦ã€‚
    """
    b_norm = b.norm()
    # é˜²æ­¢é™¤ä»¥é›¶çš„éŒ¯èª¤
    if b_norm == 0:
        return torch.tensor(0.0)
    
    # (aÂ·b) / ||b||
    return torch.dot(a, b) / b_norm

def load_vector_from_pt(vector_path: str, layer: int, device: str = 'cuda') -> torch.Tensor:
    """
    å¾ .pt æª”æ¡ˆè¼‰å…¥æŒ‡å®šå±¤æ•¸çš„å‘é‡
    
    Args:
        vector_path: .pt æª”æ¡ˆè·¯å¾‘
        layer: è¦è¼‰å…¥çš„å±¤æ•¸
        device: é‹ç®—è¨­å‚™ ('cuda' or 'cpu')
    
    Returns:
        æŒ‡å®šå±¤çš„å‘é‡ tensor
    """
    print(f"ğŸ“‚ è¼‰å…¥å‘é‡æª”æ¡ˆ: {vector_path}")
    data = torch.load(vector_path, weights_only=False)
    
    layer = layer + 1
    
    # æª¢æŸ¥æª”æ¡ˆæ ¼å¼
    if isinstance(data, dict):
        # å­—å…¸æ ¼å¼ï¼š{layer_num: tensor, ...}
        if layer not in data:
            available_layers = list(data.keys())
            raise ValueError(f"âŒ å±¤æ•¸ {layer} ä¸å­˜åœ¨æ–¼æª”æ¡ˆä¸­ã€‚å¯ç”¨å±¤æ•¸: {available_layers}")
        vector = data[layer].to(device)
    elif isinstance(data, torch.Tensor):
        # Tensor æ ¼å¼ï¼š[num_layers, hidden_dim]
        if data.dim() != 2:
            raise ValueError(f"âŒ é æœŸ 2D tensor [num_layers, hidden_dim]ï¼Œä½†å¾—åˆ° {data.shape}")
        
        num_layers, hidden_dim = data.shape
        print(f"ğŸ” æª”æ¡ˆåŒ…å« {num_layers} å±¤ï¼Œæ¯å±¤ç¶­åº¦ {hidden_dim}")
        
        if layer >= num_layers or layer < 0:
            raise ValueError(f"âŒ å±¤æ•¸ {layer} è¶…å‡ºç¯„åœã€‚å¯ç”¨å±¤æ•¸: 0-{num_layers-1}")
        
        vector = data[layer].to(device)
    else:
        raise ValueError(f"âŒ ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {type(data)}")
    
    print(f"âœ… æˆåŠŸè¼‰å…¥ç¬¬ {layer} å±¤ï¼Œå‘é‡ç¶­åº¦: {vector.shape}")
    return vector

def main():
    # --- åƒæ•¸è§£æ ---
    parser = argparse.ArgumentParser(description="è¼‰å…¥ .pt æª”æ¡ˆä¸¦é€²è¡Œä»»æ„å±¤æ•¸çš„å‘é‡æŠ•å½±é‹ç®—")
    parser.add_argument("--vector_a_path", type=str, required=True, help="å‘é‡ A çš„ .pt æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--vector_b_path", type=str, required=True, help="å‘é‡ B çš„ .pt æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--layer_list", type=int, nargs="+", default=[15], help="è¦è¨ˆç®—çš„å±¤æ•¸æ¸…å–®")
    parser.add_argument("--device", type=str, default='cuda', choices=['cuda', 'cpu'], help="é‹ç®—è¨­å‚™")
    parser.add_argument("--simulate_activation", action="store_true", help="æ˜¯å¦æ¨¡æ“¬æ¿€æ´»å‘é‡é€²è¡Œæ¸¬è©¦")
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æª”æ¡ˆå­˜åœ¨æ€§
    if not os.path.exists(args.vector_a_path):
        raise FileNotFoundError(f"âŒ å‘é‡ A æª”æ¡ˆä¸å­˜åœ¨: {args.vector_a_path}")
    if not os.path.exists(args.vector_b_path):
        raise FileNotFoundError(f"âŒ å‘é‡ B æª”æ¡ˆä¸å­˜åœ¨: {args.vector_b_path}")
    
    # è¨­å®šè¨­å‚™
    device = args.device if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ è…³æœ¬å°‡åœ¨ [ {device.upper()} ] è¨­å‚™ä¸Šé‹è¡Œ")
    print(f"ğŸ¯ ç›®æ¨™å±¤æ•¸: {args.layer_list}\n")
    
    # å°æ¯å€‹å±¤æ•¸é€²è¡Œè¨ˆç®—
    for layer in args.layer_list:
        print("=" * 80)
        print(f"ğŸ” è¨ˆç®—ç¬¬ {layer} å±¤")
        print("=" * 80)
        
        # --- è¼‰å…¥çœŸå¯¦å‘é‡ ---
        try:
            vector_a = load_vector_from_pt(args.vector_a_path, layer, device)
            vector_b = load_vector_from_pt(args.vector_b_path, layer, device)
        except Exception as e:
            print(f"âŒ è¼‰å…¥å‘é‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            continue
        
        # æ¨™æº–åŒ–å‘é‡ï¼ˆå¯é¸ï¼‰
        vector_a_normalized = vector_a / vector_a.norm()
        vector_b_normalized = vector_b / vector_b.norm()
        
        # æ¨¡æ“¬æ¿€æ´»å‘é‡ï¼ˆå¦‚æœéœ€è¦æ¸¬è©¦ï¼‰
        if args.simulate_activation:
            activation_vec = torch.randn_like(vector_a, device=device)
        else:
            # å¦‚æœä¸æ¨¡æ“¬ï¼Œå¯ä»¥åœ¨é€™è£¡åŠ å…¥å¾å…¶ä»–ä¾†æºè¼‰å…¥æ¿€æ´»å‘é‡çš„é‚è¼¯
            activation_vec = None

        print("--- å‘é‡åŸºæœ¬è³‡è¨Š ---")
        print(f"å‘é‡ A çš„é•·åº¦ (Norm): {vector_a.norm().item():.4f}")
        print(f"å‘é‡ B çš„é•·åº¦ (Norm): {vector_b.norm().item():.4f}")
        print(f"å‘é‡ A (æ¨™æº–åŒ–) çš„é•·åº¦: {vector_a_normalized.norm().item():.4f}")
        print(f"å‘é‡ B (æ¨™æº–åŒ–) çš„é•·åº¦: {vector_b_normalized.norm().item():.4f}")
        print(f"A å’Œ B çš„é¤˜å¼¦ç›¸ä¼¼åº¦: {torch.dot(vector_a_normalized, vector_b_normalized).item():.4f}\n")

        # --- æ ¸å¿ƒè¨ˆç®— ---
        print("--- æ ¸å¿ƒè¨ˆç®—çµæœ ---")

        # (A + B) çš„ç·šæ€§çµ„åˆ (ä½¿ç”¨æ¨™æº–åŒ–å‘é‡)
        vector_sum = vector_a_normalized + vector_b_normalized

        # è¨ˆç®— (A + B) æŠ•å½±åˆ° A å’Œ B
        proj_sum_on_a = a_proj_b(vector_sum, vector_a_normalized)
        proj_sum_on_b = a_proj_b(vector_sum, vector_b_normalized)

        print(f"1. (A+B) æŠ•å½±åˆ° A ä¸Šçš„ç´”é‡çµæœ: {proj_sum_on_a.item():.4f}")
        print(f"   æ•¸å­¸é©—è­‰: (A proj A) + (B proj A) = {a_proj_b(vector_a_normalized, vector_a_normalized).item():.2f} + {a_proj_b(vector_b_normalized, vector_a_normalized).item():.4f} = {a_proj_b(vector_a_normalized, vector_a_normalized).item() + a_proj_b(vector_b_normalized, vector_a_normalized).item():.4f}\n")

        print(f"2. (A+B) æŠ•å½±åˆ° B ä¸Šçš„ç´”é‡çµæœ: {proj_sum_on_b.item():.4f}")
        print(f"   æ•¸å­¸é©—è­‰: (A proj B) + (B proj B) = {a_proj_b(vector_a_normalized, vector_b_normalized).item():.4f} + {a_proj_b(vector_b_normalized, vector_b_normalized).item():.2f} = {a_proj_b(vector_a_normalized, vector_b_normalized).item() + a_proj_b(vector_b_normalized, vector_b_normalized).item():.4f}\n")

        # å¦‚æœæœ‰æ¿€æ´»å‘é‡ï¼Œé€²è¡ŒæŠ•å½±è¨ˆç®—
        if activation_vec is not None:
            # è¨ˆç®— Activation æŠ•å½±åˆ°å„å€‹å‘é‡
            proj_activation_on_a = a_proj_b(activation_vec, vector_a_normalized)
            proj_activation_on_b = a_proj_b(activation_vec, vector_b_normalized)
            proj_activation_on_sum = a_proj_b(activation_vec, vector_sum)

            print(f"3. æ¨¡å‹çš„ Activation æŠ•å½±åˆ° A: {proj_activation_on_a.item():.4f}")
            print(f"   (é€™è¡¨ç¤ºæ¨¡å‹å›æ‡‰åœ¨å¤šå¤§ç¨‹åº¦ä¸Šç¬¦åˆã€æ¦‚å¿µAã€)\n")

            print(f"4. æ¨¡å‹çš„ Activation æŠ•å½±åˆ° B: {proj_activation_on_b.item():.4f}")
            print(f"   (é€™è¡¨ç¤ºæ¨¡å‹å›æ‡‰åœ¨å¤šå¤§ç¨‹åº¦ä¸Šç¬¦åˆã€æ¦‚å¿µBã€)\n")

            print(f"5. æ¨¡å‹çš„ Activation æŠ•å½±åˆ° (A+B): {proj_activation_on_sum.item():.4f}")
            print(f"   (é€™è¡¨ç¤ºæ¨¡å‹å›æ‡‰åœ¨å¤šå¤§ç¨‹åº¦ä¸ŠåŒæ™‚ç¬¦åˆã€æ¦‚å¿µAã€å’Œã€æ¦‚å¿µBã€)\n")
        
        # è¼¸å‡ºå‘é‡åˆä½µçš„çµæœ
        print("--- å‘é‡åˆä½µåˆ†æ ---")
        merged_vector = (vector_a_normalized + vector_b_normalized) / 2  # å¹³å‡åˆä½µ
        print(f"å¹³å‡åˆä½µå‘é‡ (A+B)/2 çš„é•·åº¦: {merged_vector.norm().item():.4f}")
        print(f"åˆä½µå‘é‡èˆ‡ A çš„é¤˜å¼¦ç›¸ä¼¼åº¦: {torch.dot(merged_vector, vector_a_normalized).item():.4f}")
        print(f"åˆä½µå‘é‡èˆ‡ B çš„é¤˜å¼¦ç›¸ä¼¼åº¦: {torch.dot(merged_vector, vector_b_normalized).item():.4f}\n")
        
        print(f"âœ… ç¬¬ {layer} å±¤è¨ˆç®—å®Œæˆ\n")

if __name__ == "__main__":
    main()