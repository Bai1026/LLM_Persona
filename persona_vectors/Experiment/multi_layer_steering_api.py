from flask import Flask, request, jsonify
import argparse
import json
import os
from rich import print
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
from activation_steer import ActivationSteererMultiple
import numpy as np

app = Flask(__name__)
chatbot = None

class MultiLayerPersonaChatbot:
    """
    å¤šå±¤ç´š Persona èŠå¤©æ©Ÿå™¨äºº
    æ”¯æ´ä¸åŒ persona åœ¨ä¸åŒå±¤é€²è¡Œ activation steering
    """
    
    def __init__(self, model_name, persona_configs, debug=False):
        """
        åˆå§‹åŒ–å¤šå±¤ç´š Persona èŠå¤©æ©Ÿå™¨äºº
        
        Args:
            model_name: æ¨¡å‹åç¨±
            persona_configs: lista åŒ…å«æ¯å€‹ persona çš„é…ç½®
                æ ¼å¼: [
                    {
                        "name": "environmentalist",
                        "vector_path": "path/to/vector.npy", 
                        "layer_idx": 15,
                        "coeff": 2.0,
                        "positions": "all"
                    },
                    ...
                ]
            debug: æ˜¯å¦é–‹å•ŸåµéŒ¯æ¨¡å¼
        """
        self.model_name = model_name
        self.persona_configs = persona_configs
        self.debug = debug
        
        # è¼‰å…¥æ¨¡å‹å’Œ tokenizer
        print(f"ğŸ¤– æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # è¼‰å…¥ persona å‘é‡
        self.personas = {}
        self.steering_instructions = []
        
        for config in persona_configs:
            name = config["name"]
            vector_path = config["vector_path"]
            
            print(f"ğŸ“ è¼‰å…¥ {name} å‘é‡: {vector_path}")
            
            if not os.path.exists(vector_path):
                raise FileNotFoundError(f"å‘é‡æª”æ¡ˆä¸å­˜åœ¨: {vector_path}")
            
            # æ”¯æ´å¤šç¨®æª”æ¡ˆæ ¼å¼
            if vector_path.endswith('.pt'):
                vector = torch.load(vector_path, map_location='cpu')
                if isinstance(vector, torch.Tensor):
                    vector = vector.numpy()
                else:
                    raise ValueError(f"PyTorch æª”æ¡ˆ {vector_path} ä¸åŒ…å« tensor")
            elif vector_path.endswith('.npy'):
                vector = np.load(vector_path)
            elif vector_path.endswith('.npz'):
                npz_file = np.load(vector_path)
                # å‡è¨­ npz æª”æ¡ˆåªæœ‰ä¸€å€‹é™£åˆ—ï¼Œå–ç¬¬ä¸€å€‹
                vector = npz_file[list(npz_file.keys())[0]]
            else:
                raise ValueError(f"ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼: {vector_path}")
            self.personas[name] = {
                "vector": vector,
                "config": config
            }
            
            # å»ºç«‹ steering æŒ‡ä»¤
            instruction = {
                "steering_vector": vector,
                "layer_idx": config.get("layer_idx", 20),
                "coeff": config.get("coeff", 0.0),  # é è¨­ç‚º 0ï¼Œå¯å‹•æ…‹èª¿æ•´
                "positions": config.get("positions", "all")
            }
            self.steering_instructions.append(instruction)
        
        # å°è©±æ­·å²
        self.conversation_history = []
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.personas)} å€‹ persona")
        self._print_persona_info()
    
    def _print_persona_info(self):
        """åˆ—å° persona è³‡è¨Š"""
        print("\nğŸ“‹ Persona é…ç½®è³‡è¨Š:")
        for name, persona in self.personas.items():
            config = persona["config"]
            print(f"  â€¢ {name}: å±¤ {config.get('layer_idx', 20)}, "
                  f"ä¿‚æ•¸ {config.get('coeff', 0.0)}, "
                  f"ä½ç½® {config.get('positions', 'all')}")
    
    def update_persona_weights(self, weights):
        """
        æ›´æ–° persona æ¬Šé‡ï¼ˆä¿‚æ•¸ï¼‰
        
        Args:
            weights: dict, æ ¼å¼ {"persona_name": coeff, ...}
        """
        for i, (name, persona) in enumerate(self.personas.items()):
            if name in weights:
                new_coeff = float(weights[name])
                self.steering_instructions[i]["coeff"] = new_coeff
                persona["config"]["coeff"] = new_coeff
                
        if self.debug:
            print(f"ğŸ“Š æ›´æ–°æ¬Šé‡: {weights}")
    
    def get_current_weights(self):
        """å–å¾—ç•¶å‰æ¬Šé‡"""
        weights = {}
        for name, persona in self.personas.items():
            weights[name] = persona["config"]["coeff"]
        return weights
    
    def set_persona_mode(self, mode):
        """
        è¨­å®š persona æ¨¡å¼
        
        Args:
            mode: é è¨­æ¨¡å¼åç¨± (single, balanced, creative, analytical ç­‰)
        """
        if mode == "single_environmentalist":
            self.update_persona_weights({"environmentalist": 2.0, "creative": 0.0})
        elif mode == "single_creative":
            self.update_persona_weights({"environmentalist": 0.0, "creative": 2.0})
        elif mode == "balanced":
            self.update_persona_weights({"environmentalist": 1.0, "creative": 1.0})
        elif mode == "creative_focus":
            self.update_persona_weights({"environmentalist": 0.5, "creative": 2.0})
        elif mode == "analytical":
            self.update_persona_weights({"environmentalist": 2.0, "creative": 0.5})
        elif mode == "off":
            # é—œé–‰æ‰€æœ‰ steering
            weights = {name: 0.0 for name in self.personas.keys()}
            self.update_persona_weights(weights)
        else:
            raise ValueError(f"æœªçŸ¥æ¨¡å¼: {mode}")
        
        if self.debug:
            print(f"ğŸ­ è¨­å®šæ¨¡å¼: {mode}")
    
    def generate_response(self, user_input, max_tokens=1000):
        """ç”Ÿæˆå›æ‡‰"""
        # æ·»åŠ åˆ°å°è©±æ­·å²
        self.conversation_history.append(f"Human: {user_input}")
        
        # å»ºæ§‹ prompt
        if len(self.conversation_history) > 10:  # é™åˆ¶æ­·å²é•·åº¦
            recent_history = self.conversation_history[-10:]
        else:
            recent_history = self.conversation_history
        
        prompt = "\n".join(recent_history) + "\nAssistant:"
        
        # Tokenize
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=2048
        ).to(self.model.device)
        
        # ä½¿ç”¨ multi-layer steering
        with ActivationSteererMultiple(
            self.model, 
            self.steering_instructions, 
            debug=self.debug
        ):
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
        
        # è§£ç¢¼å›æ‡‰
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        ).strip()
        
        # æ·»åŠ åˆ°å°è©±æ­·å²
        self.conversation_history.append(f"Assistant: {response}")
        
        return response
    
    def reset_conversation(self):
        """é‡è¨­å°è©±æ­·å²"""
        self.conversation_history = []
        if self.debug:
            print("ğŸ”„ å°è©±æ­·å²å·²é‡è¨­")

# Flask API ç«¯é»
@app.route('/chat', methods=['POST'])
def chat_api():
    """API ç«¯é»ï¼šæ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥ä¸¦å›å‚³æ¨¡å‹å›æ‡‰"""
    try:
        data = request.get_json()
        if not data or 'user_input' not in data:
            return jsonify({"error": "ç¼ºå°‘ user_input åƒæ•¸"}), 400
        
        user_input = data['user_input']
        max_tokens = data.get('max_tokens', 1000)
        max_tokens = min(max_tokens, 2048)  # é™åˆ¶æœ€å¤§å€¼
        
        print(f"ğŸ§‘ User: {user_input}")
        response = chatbot.generate_response(user_input, max_tokens)
        print(f"ğŸ¤– Model: {response}")
        
        return jsonify({
            "user_input": user_input,
            "response": response,
            "current_weights": chatbot.get_current_weights(),
            "persona_configs": {name: persona["config"] for name, persona in chatbot.personas.items()},
            "status": "success"
        })
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/set_persona_weights', methods=['POST'])
def set_persona_weights():
    """è¨­å®š persona æ¬Šé‡"""
    try:
        data = request.get_json()
        weights = data.get('weights')
        
        if not weights:
            return jsonify({"error": "ç¼ºå°‘ weights åƒæ•¸"}), 400
        
        chatbot.update_persona_weights(weights)
        
        return jsonify({
            "current_weights": chatbot.get_current_weights(),
            "status": "weights updated"
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/set_persona_mode', methods=['POST'])
def set_persona_mode():
    """è¨­å®š persona æ¨¡å¼"""
    try:
        data = request.get_json()
        mode = data.get('mode')
        
        if not mode:
            return jsonify({"error": "ç¼ºå°‘ mode åƒæ•¸"}), 400
        
        chatbot.set_persona_mode(mode)
        
        return jsonify({
            "mode": mode,
            "current_weights": chatbot.get_current_weights(),
            "status": "mode updated"
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/update_layer_config', methods=['POST'])
def update_layer_config():
    """æ›´æ–°ç‰¹å®š persona çš„å±¤é…ç½®"""
    try:
        data = request.get_json()
        persona_name = data.get('persona_name')
        new_layer = data.get('layer_idx')
        
        if not persona_name or new_layer is None:
            return jsonify({"error": "ç¼ºå°‘ persona_name æˆ– layer_idx åƒæ•¸"}), 400
        
        if persona_name not in chatbot.personas:
            return jsonify({"error": f"æœªæ‰¾åˆ° persona: {persona_name}"}), 400
        
        # æ‰¾åˆ°å°æ‡‰çš„æŒ‡ä»¤ç´¢å¼•
        for i, (name, _) in enumerate(chatbot.personas.items()):
            if name == persona_name:
                chatbot.steering_instructions[i]["layer_idx"] = new_layer
                chatbot.personas[name]["config"]["layer_idx"] = new_layer
                break
        
        return jsonify({
            "persona_name": persona_name,
            "new_layer": new_layer,
            "persona_configs": {name: persona["config"] for name, persona in chatbot.personas.items()},
            "status": "layer updated"
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/reset', methods=['POST'])
def reset_conversation():
    """é‡è¨­å°è©±æ­·å²"""
    try:
        chatbot.reset_conversation()
        return jsonify({"status": "conversation reset"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/status', methods=['GET'])
def get_status():
    """å–å¾—ç•¶å‰ç‹€æ…‹"""
    persona_info = {}
    for name, persona in chatbot.personas.items():
        config = persona["config"]
        persona_info[name] = {
            "layer_idx": config.get("layer_idx", 20),
            "coeff": config.get("coeff", 0.0),
            "positions": config.get("positions", "all"),
            "vector_shape": persona["vector"].shape
        }
    
    return jsonify({
        "model_name": chatbot.model_name,
        "num_personas": len(chatbot.personas),
        "persona_info": persona_info,
        "current_weights": chatbot.get_current_weights(),
        "conversation_length": len(chatbot.conversation_history)
    })

@app.route('/available_modes', methods=['GET'])
def get_available_modes():
    """å–å¾—å¯ç”¨çš„æ¨¡å¼"""
    modes = [
        "single_environmentalist",
        "single_creative", 
        "balanced",
        "creative_focus",
        "analytical",
        "off"
    ]
    return jsonify({"available_modes": modes})

def main():
    global chatbot
    
    parser = argparse.ArgumentParser(description="Multi-Layer Persona Steering API")
    
    parser.add_argument("--model", default="meta-llama/Llama-3.1-8B-Instruct", help="æ¨¡å‹åç¨±")
    parser.add_argument("--config", required=True, help="Persona é…ç½®æª”æ¡ˆ (JSON)")
    parser.add_argument("--host", default="127.0.0.1", help="API ä¸»æ©Ÿä½å€")
    parser.add_argument("--port", type=int, default=5000, help="API é€£æ¥åŸ ")
    parser.add_argument("--debug", action="store_true", help="é–‹å•ŸåµéŒ¯æ¨¡å¼")
    
    args = parser.parse_args()
    
    # è¼‰å…¥ persona é…ç½®
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æª”æ¡ˆä¸å­˜åœ¨: {args.config}")
        return
    
    with open(args.config, 'r', encoding='utf-8') as f:
        persona_configs = json.load(f)
    
    # åˆå§‹åŒ–å¤šå±¤ç´š persona èŠå¤©æ©Ÿå™¨äºº
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¤šå±¤ç´š Persona Steering API æœå‹™...")
    print(f"ğŸ“ è¼‰å…¥ persona é…ç½®: {args.config}")
    
    chatbot = MultiLayerPersonaChatbot(
        model_name=args.model,
        persona_configs=persona_configs,
        debug=args.debug
    )
    
    print(f"âœ… API æœå‹™å•Ÿå‹•æ–¼ http://{args.host}:{args.port}")
    print("ğŸ“‹ å¯ç”¨ç«¯é»:")
    print(f"  POST /chat - å‚³é€è¨Šæ¯")
    print(f"  POST /set_persona_weights - è¨­å®šæ¬Šé‡")
    print(f"  POST /set_persona_mode - è¨­å®šé è¨­æ¨¡å¼")
    print(f"  POST /update_layer_config - æ›´æ–°å±¤é…ç½®")
    print(f"  POST /reset - é‡è¨­å°è©±")
    print(f"  GET /status - å–å¾—ç‹€æ…‹")
    print(f"  GET /available_modes - å–å¾—å¯ç”¨æ¨¡å¼")
    
    app.run(host=args.host, port=args.port, debug=False)

if __name__ == "__main__":
    main()