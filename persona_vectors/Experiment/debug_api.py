#!/usr/bin/env python3
"""
é™¤éŒ¯ç‰ˆæœ¬çš„ API - å¢åŠ æ›´è©³ç´°çš„éŒ¯èª¤è™•ç†å’Œè¨˜æ†¶é«”ç›£æ§
"""

from flask import Flask, request, jsonify
import argparse
from rich import print
import traceback
import torch
import gc
import psutil
import os
from multi_persona_handler import MultiPersonaChatbot

app = Flask(__name__)
chatbot = None

def log_memory_usage():
    """è¨˜éŒ„è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    print(f"ğŸ”§ è¨˜æ†¶é«”ä½¿ç”¨: {memory_info.rss / 1024 / 1024:.1f} MB")
    
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024
        gpu_memory_cached = torch.cuda.memory_reserved() / 1024 / 1024
        print(f"ğŸ”§ GPU è¨˜æ†¶é«”: {gpu_memory:.1f} MB allocated, {gpu_memory_cached:.1f} MB cached")

@app.route('/chat', methods=['POST'])
def chat_api():
    """API ç«¯é»ï¼šæ¥æ”¶ä½¿ç”¨è€…è¼¸å…¥ä¸¦å›å‚³æ¨¡å‹å›æ‡‰"""
    try:
        print("ğŸ”§ é–‹å§‹è™•ç†è«‹æ±‚...")
        log_memory_usage()
        
        data = request.get_json()
        if not data or 'user_input' not in data:
            return jsonify({"error": "ç¼ºå°‘ user_input åƒæ•¸"}), 400
        
        user_input = data['user_input']
        max_tokens = data.get('max_tokens', 16384)
        
        print(f"ğŸ§‘ User: {user_input}")
        print(f"ğŸ”§ Max tokens: {max_tokens}")
        
        # æ¸…ç† GPU è¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
        
        print("ğŸ”§ é–‹å§‹ç”Ÿæˆå›æ‡‰...")
        response = chatbot.generate_response(user_input, max_tokens)
        print(f"ğŸ¤– Model: {response}")
        
        log_memory_usage()
        
        return jsonify({
            "user_input": user_input,
            "response": response,
            "current_weights": chatbot.current_weights,
            "status": "success"
        })
        
    except Exception as e:
        print(f"âŒ è©³ç´°éŒ¯èª¤è³‡è¨Š:")
        print(f"   éŒ¯èª¤é¡å‹: {type(e).__name__}")
        print(f"   éŒ¯èª¤è¨Šæ¯: {str(e)}")
        print(f"   éŒ¯èª¤è¿½è¹¤:")
        traceback.print_exc()
        
        # è¨˜éŒ„è¨˜æ†¶é«”ç‹€æ³
        log_memory_usage()
        
        # å˜—è©¦é‡‹æ”¾è¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        return jsonify({
            "error": str(e),
            "error_type": type(e).__name__,
            "traceback": traceback.format_exc()
        }), 500

@app.route('/set_persona_weights', methods=['POST'])
def set_persona_weights():
    """è¨­å®š persona æ¬Šé‡"""
    try:
        data = request.get_json()
        weights = data.get('weights')
        
        if not weights:
            return jsonify({"error": "ç¼ºå°‘ weights åƒæ•¸"}), 400
        
        chatbot.update_persona_weights(weights)
        
        return jsonify({
            "current_weights": chatbot.current_weights,
            "status": "weights updated"
        })
    except Exception as e:
        print(f"âŒ è¨­å®šæ¬Šé‡éŒ¯èª¤: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/set_persona_mode', methods=['POST'])
def set_persona_mode():
    """è¨­å®š persona æ¨¡å¼"""
    try:
        data = request.get_json()
        mode = data.get('mode')
        
        if not mode:
            return jsonify({"error": "ç¼ºå°‘ mode åƒæ•¸"}), 400
        
        chatbot.set_persona_mode(mode)
        
        return jsonify({
            "mode": mode,
            "current_weights": chatbot.current_weights,
            "status": "mode updated"
        })
    except Exception as e:
        print(f"âŒ è¨­å®šæ¨¡å¼éŒ¯èª¤: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/reset', methods=['POST'])
def reset_conversation():
    """é‡è¨­å°è©±æ­·å²"""
    try:
        chatbot.reset_conversation()
        print("ğŸ”„ å°è©±æ­·å²å·²é‡è¨­")
        
        # æ¸…ç†è¨˜æ†¶é«”
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        log_memory_usage()
        return jsonify({"status": "conversation reset"})
    except Exception as e:
        print(f"âŒ é‡è¨­éŒ¯èª¤: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

@app.route('/status', methods=['GET'])
def get_status():
    """å–å¾—ç•¶å‰ç‹€æ…‹"""
    try:
        log_memory_usage()
        return jsonify({
            "model_name": chatbot.base_chatbot.model_name,
            "layer_idx": chatbot.base_chatbot.layer_idx,
            "steering_coef": chatbot.base_chatbot.steering_coef,
            "current_weights": chatbot.current_weights,
            "num_personas": len(chatbot.persona_handler.personas),
            "conversation_length": len(chatbot.base_chatbot.conversation_history)
        })
    except Exception as e:
        print(f"âŒ ç‹€æ…‹æŸ¥è©¢éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/memory', methods=['GET'])
def get_memory_status():
    """å–å¾—è¨˜æ†¶é«”ç‹€æ…‹"""
    try:
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        result = {
            "ram_usage_mb": memory_info.rss / 1024 / 1024,
            "cpu_percent": process.cpu_percent()
        }
        
        if torch.cuda.is_available():
            result.update({
                "gpu_allocated_mb": torch.cuda.memory_allocated() / 1024 / 1024,
                "gpu_cached_mb": torch.cuda.memory_reserved() / 1024 / 1024,
                "gpu_total_mb": torch.cuda.get_device_properties(0).total_memory / 1024 / 1024
            })
        
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

def main():
    global chatbot
    
    parser = argparse.ArgumentParser(description="Debug Multi-Persona Chatbot API")
    parser.add_argument("--model", default="google/gemma-2-9b-it", help="æ¨¡å‹åç¨±")
    parser.add_argument("--vector_paths", nargs='+', required=True, help="å¤šå€‹ persona å‘é‡è·¯å¾‘")
    parser.add_argument("--layer", type=int, default=20, help="ç›®æ¨™å±¤æ•¸")
    parser.add_argument("--coef", type=float, default=2.0, help="steering ä¿‚æ•¸")
    parser.add_argument("--fusion_method", default="weighted_average", 
                       choices=["weighted_average", "concatenate", "attention", "dynamic"],
                       help="å‘é‡èåˆæ–¹æ³•")
    parser.add_argument("--host", default="127.0.0.1", help="API ä¸»æ©Ÿä½å€")
    parser.add_argument("--port", type=int, default=5000, help="API é€£æ¥åŸ ")
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–å¤šé‡ persona èŠå¤©æ©Ÿå™¨äºº
        print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¤šé‡ Persona API æœå‹™...")
        print(f"ğŸ“ è¼‰å…¥ persona å‘é‡: {args.vector_paths}")
        
        log_memory_usage()
        
        chatbot = MultiPersonaChatbot(
            model_name=args.model,
            vector_paths=args.vector_paths,
            layer_idx=args.layer,
            steering_coef=args.coef,
            fusion_method=args.fusion_method
        )
        
        print(f"âœ… API æœå‹™å•Ÿå‹•æ–¼ http://{args.host}:{args.port}")
        print("ğŸ“‹ å¯ç”¨ç«¯é»:")
        print(f"  POST /chat - å‚³é€è¨Šæ¯")
        print(f"  POST /set_persona_weights - è¨­å®šæ¬Šé‡")
        print(f"  POST /set_persona_mode - è¨­å®šæ¨¡å¼")
        print(f"  POST /reset - é‡è¨­å°è©±")
        print(f"  GET /status - å–å¾—ç‹€æ…‹")
        print(f"  GET /memory - è¨˜æ†¶é«”ç‹€æ…‹")
        
        log_memory_usage()
        
        app.run(host=args.host, port=args.port, debug=False)
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()
