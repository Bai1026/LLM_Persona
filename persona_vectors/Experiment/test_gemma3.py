import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

def debug_gemma3_model():
    """é™¤éŒ¯ Gemma-3 æ¨¡å‹çš„çµæ§‹å’Œå›æ‡‰å•é¡Œ"""
    
    # è¨­å®šç’°å¢ƒè®Šæ•¸ä¾†å”åŠ©é™¤éŒ¯
    os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
    
    model_name = "google/gemma-3-4b-it"
    print(f"ğŸ” è¼‰å…¥æ¨¡å‹: {model_name}")
    
    try:
        # è¼‰å…¥åˆ†è©å™¨
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # å˜—è©¦ä½¿ç”¨æ›´ç©©å®šçš„è¼‰å…¥æ–¹å¼
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,  # ä½¿ç”¨ bfloat16 è€Œé float16
            device_map="auto",
            trust_remote_code=True,
            attn_implementation="flash_attention_2" if torch.cuda.is_available() else "eager"
        )
        
        # è¨­å®š pad token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print(f"ğŸ“Š æ¨¡å‹é¡å‹: {type(model).__name__}")
        print(f"ğŸ“Š æ¨¡å‹è¨­å‚™: {next(model.parameters()).device}")
        print(f"ğŸ“Š æ¨¡å‹ç²¾åº¦: {next(model.parameters()).dtype}")
        print(f"ğŸ“Š åˆ†è©å™¨è³‡è¨Š: pad_token={tokenizer.pad_token}, eos_token={tokenizer.eos_token}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        # å›é€€åˆ° CPU å’Œ float32
        print("ğŸ”„ å˜—è©¦å›é€€åˆ° CPU å’Œ float32...")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            device_map="cpu",
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        print(f"ğŸ“Š å›é€€æ¨¡å‹é¡å‹: {type(model).__name__}")
        print(f"ğŸ“Š å›é€€æ¨¡å‹è¨­å‚™: {next(model.parameters()).device}")
        print(f"ğŸ“Š å›é€€æ¨¡å‹ç²¾åº¦: {next(model.parameters()).dtype}")
    
    # æª¢æŸ¥æ¨¡å‹çµæ§‹
    print("\nğŸ—ï¸ æ¨¡å‹çµæ§‹:")
    print(f"  - ç¸½å±¤æ•¸: {len(model.model.language_model.layers)}")
    print(f"  - ç¬¬ä¸€å±¤é¡å‹: {type(model.model.language_model.layers[0]).__name__}")
    
    # æ¸¬è©¦åŸºæœ¬æ¨ç†ï¼ˆä½¿ç”¨æœ€ä¿å®ˆçš„åƒæ•¸ï¼‰
    print("\nğŸ§ª æ¸¬è©¦åŸºæœ¬æ¨ç†:")
    test_prompt = "Hello"
    
    try:
        inputs = tokenizer(
            test_prompt, 
            return_tensors="pt",
            add_special_tokens=True,
            max_length=8192,  # æ˜ç¢ºè¨­å®šæœ€å¤§é•·åº¦
            truncation=True,
            padding=False
        )
        
        # ç§»å‹•åˆ°ç›¸åŒè¨­å‚™
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # åŠ å…¥æ³¨æ„åŠ›é®ç½©
        if 'attention_mask' not in inputs:
            inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])
        
        with torch.no_grad():
            # ä½¿ç”¨æœ€ç°¡å–®çš„è²ªå©ªè§£ç¢¼
            outputs = model.generate(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask'],
                max_new_tokens=10,
                do_sample=False,  # è²ªå©ªè§£ç¢¼
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                use_cache=True,
                output_scores=False,
                return_dict_in_generate=False
            )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"âœ… è¼¸å…¥: {test_prompt}")
            print(f"âœ… è¼¸å‡º: {response}")
            
    except Exception as e:
        print(f"âŒ åŸºæœ¬æ¨ç†å¤±æ•—: {e}")
        print("ğŸ” å˜—è©¦æ›´ç°¡å–®çš„æ¸¬è©¦...")
        
        # æ›´ç°¡å–®çš„æ¸¬è©¦ - ç›´æ¥å‰å‘å‚³æ’­
        try:
            inputs = tokenizer("Hello", return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                logits = model(**inputs).logits
                next_token_id = torch.argmax(logits[0, -1, :], dim=-1)
                next_token = tokenizer.decode(next_token_id)
                print(f"âœ… å‰å‘å‚³æ’­æˆåŠŸï¼Œä¸‹ä¸€å€‹ token: '{next_token}'")
                
        except Exception as e2:
            print(f"âŒ å‰å‘å‚³æ’­ä¹Ÿå¤±æ•—: {e2}")
            return None, None
    
    # æ¸¬è©¦ç°¡å–®çš„å°è©±æ ¼å¼
    print("\nğŸ­ æ¸¬è©¦ç°¡å–®å°è©±:")
    try:
        conversation = [
            {"role": "user", "content": "What is art?"},
        ]
        
        prompt = tokenizer.apply_chat_template(
            conversation, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=30,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
            
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"âœ… å°è©±æ¸¬è©¦æˆåŠŸ")
            print(f"   å®Œæ•´å›æ‡‰: {response}")
            
    except Exception as e:
        print(f"âŒ å°è©±æ¸¬è©¦å¤±æ•—: {e}")
    
    return model, tokenizer

def test_different_precision():
    """æ¸¬è©¦ä¸åŒç²¾åº¦è¨­å®š"""
    model_name = "google/gemma-3-4b-it"
    
    precisions = [
        ("float32", torch.float32),
        ("bfloat16", torch.bfloat16),
        ("float16", torch.float16),
    ]
    
    for name, dtype in precisions:
        print(f"\nğŸ§® æ¸¬è©¦ {name} ç²¾åº¦:")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=dtype,
                device_map="auto" if dtype != torch.float32 else "cpu"
            )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # ç°¡å–®æ¸¬è©¦
            inputs = tokenizer("Hello", return_tensors="pt")
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=5,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id
                )
                
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"âœ… {name} æˆåŠŸ: {response}")
            
            # æ¸…ç†è¨˜æ†¶é«”
            del model
            torch.cuda.empty_cache()
            
        except Exception as e:
            print(f"âŒ {name} å¤±æ•—: {e}")

if __name__ == "__main__":
    print("ğŸ”¬ é–‹å§‹ Gemma-3 è¨ºæ–·...")
    
    # å…ˆæ¸¬è©¦ä¸åŒç²¾åº¦
    test_different_precision()
    
    print("\n" + "="*50)
    print("ğŸ¯ è©³ç´°æ¨¡å‹æ¸¬è©¦:")
    
    # è©³ç´°æ¸¬è©¦
    model, tokenizer = debug_gemma3_model()
    
    if model is not None:
        print("\nâœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œå¯ä»¥é€²è¡Œå¾ŒçºŒçš„ eval_persona æ•´åˆ")
    else:
        print("\nâŒ æ¨¡å‹è¼‰å…¥å¤±æ•—ï¼Œéœ€è¦é€²ä¸€æ­¥èª¿æŸ¥")