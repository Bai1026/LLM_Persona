import torch
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns
import pandas as pd
import os

# è¨­å®š matplotlib ä½¿ç”¨è‹±æ–‡
plt.rcParams['font.family'] = ['DejaVu Sans', 'Liberation Sans']
plt.rcParams['axes.unicode_minus'] = False

def ensure_directory_exists(directory_path):
    """ç¢ºä¿ç›®éŒ„å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å»ºç«‹"""
    if not os.path.exists(directory_path):
        os.makedirs(directory_path)
        print(f"ğŸ“ å»ºç«‹ç›®éŒ„: {directory_path}")

class VectorAnalyzer:
    """å‘é‡åˆ†æå™¨"""
    
    def __init__(self, vector_path: str):
        self.vector_path = vector_path
        self.vector = torch.load(vector_path, weights_only=False)
        print(f"ğŸ“Š è¼‰å…¥å‘é‡: {vector_path}")
        print(f"ğŸ“ å‘é‡å½¢ç‹€: {self.vector.shape}")
    
    def basic_statistics(self):
        """åŸºæœ¬çµ±è¨ˆè³‡è¨Š"""
        print(f"\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ:")
        print(f"   ç¸½å…ƒç´ æ•¸: {self.vector.numel():,}")
        print(f"   å¹³å‡å€¼: {self.vector.mean().item():.6f}")
        print(f"   æ¨™æº–å·®: {self.vector.std().item():.6f}")
        print(f"   æœ€å°å€¼: {self.vector.min().item():.6f}")
        print(f"   æœ€å¤§å€¼: {self.vector.max().item():.6f}")
        print(f"   éé›¶å…ƒç´ : {(self.vector != 0).sum().item():,}")
        print(f"   éé›¶æ¯”ä¾‹: {(self.vector != 0).float().mean().item():.2%}")
        
        return {
            'mean': self.vector.mean().item(),
            'std': self.vector.std().item(),
            'min': self.vector.min().item(),
            'max': self.vector.max().item(),
            'non_zero_ratio': (self.vector != 0).float().mean().item()
        }
    
    def plot_layer_statistics(self, save_path: str = None):
        """ç¹ªè£½æ¯å±¤çš„çµ±è¨ˆè³‡è¨Š"""
        num_layers = self.vector.shape[0]
        
        # è¨ˆç®—æ¯å±¤çš„çµ±è¨ˆè³‡è¨Š
        layer_stats = []
        for i in range(num_layers):
            layer_vector = self.vector[i]
            stats = {
                'layer': i,
                'mean': layer_vector.mean().item(),
                'std': layer_vector.std().item(),
                'l2_norm': torch.norm(layer_vector, p=2).item(),
                'max_abs': layer_vector.abs().max().item()
            }
            layer_stats.append(stats)
        
        df = pd.DataFrame(layer_stats)
        
        # å»ºç«‹è¦–è¦ºåŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Layer-wise Vector Statistics', fontsize=16)
        
        # å¹³å‡å€¼
        axes[0, 0].plot(df['layer'], df['mean'], 'b-o', markersize=4)
        axes[0, 0].set_title('Mean Value per Layer')
        axes[0, 0].set_xlabel('Layer Index')
        axes[0, 0].set_ylabel('Mean Value')
        axes[0, 0].grid(True, alpha=0.3)
        
        # æ¨™æº–å·®
        axes[0, 1].plot(df['layer'], df['std'], 'r-o', markersize=4)
        axes[0, 1].set_title('Standard Deviation per Layer')
        axes[0, 1].set_xlabel('Layer Index')
        axes[0, 1].set_ylabel('Standard Deviation')
        axes[0, 1].grid(True, alpha=0.3)
        
        # L2 ç¯„æ•¸
        axes[1, 0].plot(df['layer'], df['l2_norm'], 'g-o', markersize=4)
        axes[1, 0].set_title('L2 Norm per Layer')
        axes[1, 0].set_xlabel('Layer Index')
        axes[1, 0].set_ylabel('L2 Norm')
        axes[1, 0].grid(True, alpha=0.3)
        
        # æœ€å¤§çµ•å°å€¼
        axes[1, 1].plot(df['layer'], df['max_abs'], 'purple', marker='o', markersize=4)
        axes[1, 1].set_title('Max Absolute Value per Layer')
        axes[1, 1].set_xlabel('Layer Index')
        axes[1, 1].set_ylabel('Max Absolute Value')
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            # ç¢ºä¿ fig ç›®éŒ„å­˜åœ¨
            fig_dir = os.path.join(os.path.dirname(save_path), 'fig')
            ensure_directory_exists(fig_dir)
            
            # ä¿®æ”¹å„²å­˜è·¯å¾‘åˆ° fig è³‡æ–™å¤¾
            filename = os.path.basename(save_path)
            fig_save_path = os.path.join(fig_dir, filename)
            plt.savefig(fig_save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ åœ–è¡¨å·²å„²å­˜: {fig_save_path}")
        
        plt.show()
        
        return df
    
    def plot_value_distribution(self, layer_idx: int = 20, save_path: str = None):
        """ç¹ªè£½ç‰¹å®šå±¤çš„æ•¸å€¼åˆ†ä½ˆ"""
        if layer_idx >= self.vector.shape[0]:
            layer_idx = self.vector.shape[0] - 1
        
        layer_vector = self.vector[layer_idx].flatten().numpy()
        
        fig, axes = plt.subplots(1, 2, figsize=(15, 5))
        fig.suptitle(f'Layer {layer_idx} Value Distribution', fontsize=14)
        
        # ç›´æ–¹åœ–
        axes[0].hist(layer_vector, bins=50, alpha=0.7, edgecolor='black')
        axes[0].set_title('Value Distribution Histogram')
        axes[0].set_xlabel('Vector Values')
        axes[0].set_ylabel('Frequency')
        axes[0].grid(True, alpha=0.3)
        
        # ç®±ç·šåœ–
        axes[1].boxplot(layer_vector, vert=True)
        axes[1].set_title('Value Distribution Boxplot')
        axes[1].set_ylabel('Vector Values')
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            # ç¢ºä¿ fig ç›®éŒ„å­˜åœ¨
            fig_dir = os.path.join(os.path.dirname(save_path), 'fig')
            ensure_directory_exists(fig_dir)
            
            # ä¿®æ”¹å„²å­˜è·¯å¾‘åˆ° fig è³‡æ–™å¤¾
            filename = os.path.basename(save_path)
            fig_save_path = os.path.join(fig_dir, filename)
            plt.savefig(fig_save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ åœ–è¡¨å·²å„²å­˜: {fig_save_path}")
        
        plt.show()
    
    def plot_heatmap(self, layer_range: tuple = (15, 25), save_path: str = None):
        """ç¹ªè£½å‘é‡ç†±åŠ›åœ–"""
        start_layer, end_layer = layer_range
        end_layer = min(end_layer, self.vector.shape[0])
        
        # é¸æ“‡éƒ¨åˆ†å±¤å’Œç¶­åº¦ä¾†å±•ç¤º
        selected_layers = self.vector[start_layer:end_layer]
        
        # å¦‚æœç¶­åº¦å¤ªå¤§ï¼Œæ¡æ¨£éƒ¨åˆ†ç¶­åº¦
        if selected_layers.shape[1] > 100:
            sample_dims = torch.randperm(selected_layers.shape[1])[:100]
            selected_layers = selected_layers[:, sample_dims]
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(
            selected_layers.numpy(), 
            cmap='RdBu_r', 
            center=0,
            xticklabels=False,
            yticklabels=[f'Layer {i}' for i in range(start_layer, end_layer)]
        )
        plt.title(f'Vector Heatmap (Layers {start_layer}-{end_layer-1})')
        plt.xlabel('Vector Dimensions')
        plt.ylabel('Model Layers')
        
        if save_path:
            # ç¢ºä¿ fig ç›®éŒ„å­˜åœ¨
            fig_dir = os.path.join(os.path.dirname(save_path), 'fig')
            ensure_directory_exists(fig_dir)
            
            # ä¿®æ”¹å„²å­˜è·¯å¾‘åˆ° fig è³‡æ–™å¤¾
            filename = os.path.basename(save_path)
            fig_save_path = os.path.join(fig_dir, filename)
            plt.savefig(fig_save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ åœ–è¡¨å·²å„²å­˜: {fig_save_path}")
        
        plt.show()

class MultiVectorComparator:
    """å¤šå‘é‡æ¯”è¼ƒå™¨"""
    
    def __init__(self, vector_dir: str):
        self.vector_dir = vector_dir
        self.vectors = {}
        self.load_all_vectors()
    
    def load_all_vectors(self):
        """è¼‰å…¥æ‰€æœ‰å‘é‡"""
        if not os.path.exists(self.vector_dir):
            print(f"âŒ å‘é‡ç›®éŒ„ä¸å­˜åœ¨: {self.vector_dir}")
            return
        
        for filename in os.listdir(self.vector_dir):
            if filename.endswith("_response_avg_diff.pt"):
                role_name = filename.replace("_response_avg_diff.pt", "")
                vector_path = os.path.join(self.vector_dir, filename)
                
                try:
                    self.vectors[role_name] = torch.load(vector_path, weights_only=False)
                    print(f"ğŸ“Š è¼‰å…¥å‘é‡: {role_name} - {self.vectors[role_name].shape}")
                except Exception as e:
                    print(f"âŒ è¼‰å…¥ {role_name} å‘é‡å¤±æ•—: {e}")
    
    def compare_statistics(self):
        """æ¯”è¼ƒæ‰€æœ‰å‘é‡çš„çµ±è¨ˆè³‡è¨Š"""
        if not self.vectors:
            print("âš ï¸  æ²’æœ‰å‘é‡å¯ä»¥æ¯”è¼ƒ")
            return None
        
        comparison_data = []
        
        for role_name, vector in self.vectors.items():
            stats = {
                'role': role_name,
                'mean': vector.mean().item(),
                'std': vector.std().item(),
                'l2_norm': torch.norm(vector, p=2).item(),
                'max_abs': vector.abs().max().item(),
                'non_zero_ratio': (vector != 0).float().mean().item()
            }
            comparison_data.append(stats)
        
        df = pd.DataFrame(comparison_data)
        
        # è¦–è¦ºåŒ–æ¯”è¼ƒ
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        fig.suptitle('Multi-Role Vector Statistics Comparison', fontsize=16)
        
        metrics = ['mean', 'std', 'l2_norm', 'max_abs', 'non_zero_ratio']
        metric_names = ['Mean Value', 'Standard Deviation', 'L2 Norm', 'Max Absolute', 'Non-zero Ratio']
        
        for i, (metric, name) in enumerate(zip(metrics, metric_names)):
            row = i // 3
            col = i % 3
            
            bars = axes[row, col].bar(df['role'], df[metric])
            axes[row, col].set_title(name)
            axes[row, col].tick_params(axis='x', rotation=45)
            
            # ç‚ºæ¯å€‹æ¢å½¢æ·»åŠ æ•¸å€¼æ¨™ç±¤
            for bar in bars:
                height = bar.get_height()
                axes[row, col].text(bar.get_x() + bar.get_width()/2., height,
                                   f'{height:.4f}', ha='center', va='bottom')
        
        # éš±è—ç©ºç™½å­åœ–
        axes[1, 2].set_visible(False)
        
        plt.tight_layout()
        
        # å„²å­˜åœ–è¡¨åˆ° fig è³‡æ–™å¤¾
        fig_dir = os.path.join(self.vector_dir, 'fig')
        ensure_directory_exists(fig_dir)
        fig_save_path = os.path.join(fig_dir, 'multi_role_statistics_comparison.png')
        plt.savefig(fig_save_path, dpi=300, bbox_inches='tight')
        print(f"ï¿½ åœ–è¡¨å·²å„²å­˜: {fig_save_path}")
        
        print("ï¿½ğŸ“Š é¡¯ç¤ºå¤šè§’è‰²çµ±è¨ˆæ¯”è¼ƒåœ–è¡¨...")
        plt.show()
        
        return df
    
    def compute_similarity_matrix(self, layer_idx: int = 20):
        """è¨ˆç®—å‘é‡ç›¸ä¼¼åº¦çŸ©é™£"""
        if not self.vectors:
            print("âš ï¸  æ²’æœ‰å‘é‡å¯ä»¥æ¯”è¼ƒ")
            return None
        
        role_names = list(self.vectors.keys())
        
        # å¦‚æœåªæœ‰ä¸€å€‹å‘é‡ï¼Œå»ºç«‹è‡ªç›¸ä¼¼åº¦çŸ©é™£
        if len(role_names) == 1:
            similarity_matrix = np.array([[1.0]])
        else:
            similarity_matrix = np.zeros((len(role_names), len(role_names)))
            
            for i, role1 in enumerate(role_names):
                for j, role2 in enumerate(role_names):
                    vec1 = self.vectors[role1][layer_idx].flatten()
                    vec2 = self.vectors[role2][layer_idx].flatten()
                    
                    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
                    similarity = torch.cosine_similarity(
                        vec1.unsqueeze(0), vec2.unsqueeze(0)
                    ).item()
                    similarity_matrix[i, j] = similarity
        
        # ç¹ªè£½ç›¸ä¼¼åº¦ç†±åŠ›åœ–
        plt.figure(figsize=(8, 6))
        sns.heatmap(
            similarity_matrix,
            annot=True,
            fmt='.3f',
            xticklabels=role_names,
            yticklabels=role_names,
            cmap='coolwarm',
            center=0
        )
        plt.title(f'Layer {layer_idx} Vector Similarity Matrix')
        plt.tight_layout()
        
        # å„²å­˜åœ–è¡¨åˆ° fig è³‡æ–™å¤¾
        fig_dir = os.path.join(self.vector_dir, 'fig')
        ensure_directory_exists(fig_dir)
        fig_save_path = os.path.join(fig_dir, f'similarity_matrix_layer_{layer_idx}.png')
        plt.savefig(fig_save_path, dpi=300, bbox_inches='tight')
        print(f"ï¿½ åœ–è¡¨å·²å„²å­˜: {fig_save_path}")
        
        print("ï¿½ğŸ” é¡¯ç¤ºå‘é‡ç›¸ä¼¼åº¦çŸ©é™£...")
        plt.show()
        
        return similarity_matrix, role_names
    
    def pca_visualization(self, layer_idx: int = 20):
        """ä½¿ç”¨ PCA è¦–è¦ºåŒ–å‘é‡"""
        if len(self.vectors) < 2:
            print("âš ï¸  éœ€è¦è‡³å°‘ 2 å€‹å‘é‡ä¾†é€²è¡Œ PCA åˆ†æ")
            return None
        
        # æº–å‚™è³‡æ–™
        vectors_list = []
        labels = []
        
        for role_name, vector in self.vectors.items():
            vectors_list.append(vector[layer_idx].flatten().numpy())
            labels.append(role_name)
        
        vectors_array = np.stack(vectors_list)
        
        # åŸ·è¡Œ PCA
        pca = PCA(n_components=2)
        vectors_2d = pca.fit_transform(vectors_array)
        
        # ç¹ªè£½ PCA çµæœ
        plt.figure(figsize=(10, 8))
        colors = plt.cm.Set3(np.linspace(0, 1, len(labels)))
        
        for i, (label, color) in enumerate(zip(labels, colors)):
            plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1], 
                       s=200, alpha=0.8, color=color, label=label)
            plt.annotate(label, (vectors_2d[i, 0], vectors_2d[i, 1]), 
                        xytext=(5, 5), textcoords='offset points', fontsize=12)
        
        plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
        plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
        plt.title(f'Layer {layer_idx} Vector PCA Visualization')
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # å„²å­˜åœ–è¡¨åˆ° fig è³‡æ–™å¤¾
        fig_dir = os.path.join(self.vector_dir, 'fig')
        ensure_directory_exists(fig_dir)
        fig_save_path = os.path.join(fig_dir, f'pca_visualization_layer_{layer_idx}.png')
        plt.savefig(fig_save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ’¾ åœ–è¡¨å·²å„²å­˜: {fig_save_path}")
        
        print("ğŸ¯ é¡¯ç¤º PCA è¦–è¦ºåŒ–åœ–è¡¨...")
        plt.show()
        
        return vectors_2d, pca

def analyze_vector(vector_path: str):
    """å®Œæ•´åˆ†æå–®ä¸€å‘é‡"""
    print(f"ğŸ” åˆ†æå‘é‡: {vector_path}")
    
    analyzer = VectorAnalyzer(vector_path)
    
    # åŸºæœ¬çµ±è¨ˆ
    stats = analyzer.basic_statistics()

    print("\nğŸ“Š ç”¢ç”Ÿåœ–è¡¨ 1: æ¯å±¤çµ±è¨ˆè³‡è¨Š...")
    # ç¹ªè£½å±¤çµ±è¨ˆ
    layer_df = analyzer.plot_layer_statistics(
        save_path=vector_path.replace('.pt', '_layer_stats.png')
    )
    
    print("\nğŸ“Š ç”¢ç”Ÿåœ–è¡¨ 2: æ•¸å€¼åˆ†ä½ˆ...")
    # ç¹ªè£½æ•¸å€¼åˆ†ä½ˆ
    analyzer.plot_value_distribution(
        layer_idx=20,
        save_path=vector_path.replace('.pt', '_distribution.png')
    )
    
    print("\nğŸ“Š ç”¢ç”Ÿåœ–è¡¨ 3: ç†±åŠ›åœ–...")
    # ç¹ªè£½ç†±åŠ›åœ–
    analyzer.plot_heatmap(
        save_path=vector_path.replace('.pt', '_heatmap.png')
    )
    
    return analyzer, stats, layer_df

def quick_vector_check(vector_path: str):
    """å¿«é€Ÿæª¢æŸ¥å‘é‡å“è³ª"""
    try:
        # è¼‰å…¥å‘é‡
        vector = torch.load(vector_path, weights_only=False)
        print(f"âœ… æˆåŠŸè¼‰å…¥å‘é‡: {vector_path}")
        print(f"ğŸ“ å½¢ç‹€: {vector.shape}")
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   å¹³å‡å€¼: {vector.mean().item():.6f}")
        print(f"   æ¨™æº–å·®: {vector.std().item():.6f}")
        print(f"   ç¯„åœ: [{vector.min().item():.6f}, {vector.max().item():.6f}]")
        print(f"   éé›¶æ¯”ä¾‹: {(vector != 0).float().mean().item():.2%}")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºé›¶å‘é‡
        if torch.allclose(vector, torch.zeros_like(vector), atol=1e-6):
            print("âš ï¸  è­¦å‘Š: é€™ä¼¼ä¹æ˜¯é›¶å‘é‡ï¼")
            return False
        else:
            print("âœ… å‘é‡åŒ…å«æœ‰æ„ç¾©çš„æ•¸å€¼")
            return True
            
    except Exception as e:
        print(f"âŒ è¼‰å…¥å‘é‡å¤±æ•—: {e}")
        return False

def compare_all_vectors():
    """æ¯”è¼ƒæ‰€æœ‰å‘é‡"""
    vector_dir = "persona_vectors/Qwen2.5-7B-Instruct/multi_role/"
    
    comparator = MultiVectorComparator(vector_dir)
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(comparator.vectors)} å€‹å‘é‡")
    
    if len(comparator.vectors) == 0:
        print("âš ï¸  æ²’æœ‰æ‰¾åˆ°ä»»ä½•å‘é‡æª”æ¡ˆ")
        return None, None
    
    print("\nğŸ“Š ç”¢ç”Ÿåœ–è¡¨ 4: çµ±è¨ˆæ¯”è¼ƒ...")
    # çµ±è¨ˆæ¯”è¼ƒ
    stats_df = comparator.compare_statistics()
    print(f"ğŸ“‹ çµ±è¨ˆæ¯”è¼ƒçµæœ:\n{stats_df}")
    
    # å„²å­˜ CSV åˆ° csv è³‡æ–™å¤¾
    csv_dir = os.path.join(vector_dir, 'csv')
    ensure_directory_exists(csv_dir)
    csv_save_path = os.path.join(csv_dir, "comparison_stats.csv")
    stats_df.to_csv(csv_save_path, index=False)
    print(f"ğŸ’¾ çµ±è¨ˆæ¯”è¼ƒçµæœå·²å„²å­˜: {csv_save_path}")

    print("\nğŸ“Š ç”¢ç”Ÿåœ–è¡¨ 5: ç›¸ä¼¼åº¦çŸ©é™£...")
    # ç›¸ä¼¼åº¦çŸ©é™£
    sim_matrix, role_names = comparator.compute_similarity_matrix(layer_idx=20)
    if sim_matrix is not None:
        similarity_df = pd.DataFrame(sim_matrix, index=role_names, columns=role_names)
        print(f"ğŸ“‹ ç›¸ä¼¼åº¦çŸ©é™£:\n{similarity_df}")
        
        # å„²å­˜ CSV åˆ° csv è³‡æ–™å¤¾
        csv_save_path = os.path.join(csv_dir, "similarity_matrix.csv")
        similarity_df.to_csv(csv_save_path)
        print(f"ğŸ’¾ ç›¸ä¼¼åº¦çŸ©é™£å·²å„²å­˜: {csv_save_path}")
    
    # PCA è¦–è¦ºåŒ–ï¼ˆå¦‚æœæœ‰å¤šå€‹å‘é‡ï¼‰
    if len(comparator.vectors) >= 2:
        print("\nğŸ“Š ç”¢ç”Ÿåœ–è¡¨ 6: PCA è¦–è¦ºåŒ–...")
        pca_result = comparator.pca_visualization(layer_idx=20)
        if pca_result is not None:
            vectors_2d, pca = pca_result
            pca_df = pd.DataFrame(vectors_2d, index=role_names, columns=['PC1', 'PC2'])
            print(f"ğŸ“‹ PCA çµæœ (å‰å…©å€‹ä¸»æˆåˆ†):\n{pca_df}")
            
            # å„²å­˜ CSV åˆ° csv è³‡æ–™å¤¾
            csv_save_path = os.path.join(csv_dir, "pca_results.csv")
            pca_df.to_csv(csv_save_path)
            print(f"ğŸ’¾ PCA çµæœå·²å„²å­˜: {csv_save_path}")
    
    return comparator, stats_df

if __name__ == "__main__":
    print("ğŸ¯ é–‹å§‹å‘é‡åˆ†ææµç¨‹...")
    
    # åˆ†æå–®ä¸€å‘é‡
    vector_path = "persona_vectors/Qwen2.5-7B-Instruct/multi_role/empathetic_counselor_response_avg_diff.pt"
    
    if os.path.exists(vector_path):
        print("ğŸ” æ­¥é©Ÿ 1: åˆ†æå–®ä¸€å‘é‡...")
        
        # å¿«é€Ÿæª¢æŸ¥
        is_valid = quick_vector_check(vector_path)
        
        if is_valid:
            # è©³ç´°åˆ†æ (3å€‹åœ–è¡¨)
            analyzer, stats, layer_df = analyze_vector(vector_path)
        else:
            print("âš ï¸  å‘é‡å“è³ªæœ‰å•é¡Œï¼Œè·³éè©³ç´°åˆ†æ")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°å‘é‡æª”æ¡ˆ: {vector_path}")
    
    # æ¯”è¼ƒå¤šå€‹å‘é‡
    print("\nğŸ” æ­¥é©Ÿ 2: æ¯”è¼ƒå¤šå€‹å‘é‡...")
    comparator, stats_df = compare_all_vectors()
    
    print("\nâœ… åˆ†æå®Œæˆï¼æ‚¨æ‡‰è©²çœ‹åˆ°ç¸½å…± 5-6 å€‹åœ–è¡¨")
    print("ğŸ“Š åœ–è¡¨èªªæ˜:")
    print("   1. Layer-wise Vector Statistics (2x2 ç¶²æ ¼)")
    print("   2. Layer 20 Value Distribution (1x2 ç¶²æ ¼)")  
    print("   3. Vector Heatmap")
    print("   4. Multi-Role Vector Statistics Comparison (2x3 ç¶²æ ¼)")
    print("   5. Layer 20 Vector Similarity Matrix")
    if comparator and len(comparator.vectors) >= 2:
        print("   6. Layer 20 Vector PCA Visualization")
    
    print("\nğŸ“ æª”æ¡ˆå„²å­˜ä½ç½®:")
    print("   ğŸ–¼ï¸  åœ–è¡¨: persona_vectors/Qwen2.5-7B-Instruct/multi_role/fig/")
    print("   ğŸ“Š CSV: persona_vectors/Qwen2.5-7B-Instruct/multi_role/csv/")