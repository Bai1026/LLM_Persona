#!/usr/bin/env python3
"""
æ•´ç† Result è³‡æ–™å¤¾ä¸­æ‰€æœ‰ comprehensive è©•ä¼°çµæœçš„ trait_averages
"""

import json
import os
import pandas as pd
from pathlib import Path
import argparse

def extract_info_from_filename(filename):
    """å¾æª”åä¸­æå–è³‡è¨Š"""
    # ç§»é™¤å‰¯æª”å
    name = filename.replace('.json', '')
    
    # åªè™•ç† comprehensive çµæœï¼Œç§»é™¤ç›¸æ‡‰å¾Œç¶´
    name = name.replace('_evaluation_results_comprehensive', '')
    
    # æå–æ¨¡å‹åç¨±ï¼ˆå¦‚æœæœ‰ï¼‰
    model = "local"  # é è¨­
    if name.endswith('_gpt'):
        model = "gpt"
        name = name.replace('_gpt', '')
    elif name.endswith('_gemini'):
        model = "gemini"
        name = name.replace('_gemini', '')
    
    return name, model

def load_all_results(result_dir):
    """è¼‰å…¥æ‰€æœ‰çµæœæª”æ¡ˆ"""
    results = []
    result_path = Path(result_dir)
    
    if not result_path.exists():
        print(f"âŒ çµæœè³‡æ–™å¤¾ä¸å­˜åœ¨: {result_dir}")
        return results
    
    json_files = list(result_path.glob("*.json"))
    print(f"ğŸ“ æ‰¾åˆ° {len(json_files)} å€‹ JSON æª”æ¡ˆ")
    
    for file_path in json_files:
        # åªè™•ç† comprehensive é¡å‹çš„æª”æ¡ˆ
        if '_evaluation_results_comprehensive' not in file_path.name:
            print(f"â­ï¸  è·³éé comprehensive æª”æ¡ˆ: {file_path.name}")
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ trait_averages
            if 'trait_averages' not in data:
                print(f"âš ï¸  {file_path.name} æ²’æœ‰ trait_averages æ¬„ä½")
                continue
            
            # æå–æª”åè³‡è¨Š
            persona_combo, model = extract_info_from_filename(file_path.name)
            
            # å»ºç«‹çµæœè¨˜éŒ„
            result = {
                'filename': file_path.name,
                'persona_combination': persona_combo,
                'evaluation_model': model,
                **data['trait_averages']
            }
            
            results.append(result)
            print(f"âœ… è¼‰å…¥: {file_path.name} -> {persona_combo} ({model})")
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥ {file_path.name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    return results

def create_summary_table(results):
    """å»ºç«‹æ‘˜è¦è¡¨æ ¼"""
    if not results:
        print("âŒ æ²’æœ‰å¯ç”¨çš„çµæœè³‡æ–™")
        return None, None
    
    # è½‰æ›ç‚º DataFrame
    df = pd.DataFrame(results)
    
    # é‡æ–°æ’åˆ—æ¬„ä½é †åº
    trait_columns = ['empathetic', 'analytical', 'creative', 'environmental', 'futurist']
    other_columns = ['filename', 'persona_combination', 'evaluation_model']
    
    # ç¢ºä¿æ‰€æœ‰ trait æ¬„ä½éƒ½å­˜åœ¨
    for trait in trait_columns:
        if trait not in df.columns:
            df[trait] = 0.0
    
    # é‡æ–°æ’åºæ¬„ä½
    df = df[other_columns + trait_columns]
    
    # å»ºç«‹çµ±è¨ˆè¡¨æ ¼
    stats_df = create_prefix_statistics(df, trait_columns)
    
    return df, stats_df

def create_prefix_statistics(df, trait_columns):
    """å»ºç«‹ç›¸åŒå‰ç¶´çš„çµ±è¨ˆè¡¨æ ¼"""
    # å–å¾—æ‰€æœ‰å”¯ä¸€çš„ persona å‰ç¶´
    prefixes = set()
    for combo in df['persona_combination'].unique():
        # å‰ç¶´æ˜¯æ•´å€‹ persona_combinationï¼Œå› ç‚ºå®ƒå·²ç¶“æ˜¯å¾æª”åè§£æå‡ºä¾†çš„å‰ç¶´éƒ¨åˆ†
        # ï¼ˆåœ¨ extract_info_from_filename ä¸­å·²ç¶“ç§»é™¤äº† _evaluation_results_comprehensiveï¼‰
        prefixes.add(combo)
    
    stats_results = []
    
    for prefix in sorted(prefixes):
        # æ‰¾å‡ºæ‰€æœ‰å®Œå…¨åŒ¹é…æ­¤å‰ç¶´çš„çµ„åˆ
        prefix_data = df[df['persona_combination'] == prefix]
        
        if len(prefix_data) == 0:
            continue
            
        # æŒ‰è©•ä¼°æ¨¡å‹åˆ†çµ„è¨ˆç®—å¹³å‡
        for model in prefix_data['evaluation_model'].unique():
            model_data = prefix_data[prefix_data['evaluation_model'] == model]
            
            if len(model_data) > 0:
                result = {
                    'prefix': prefix,
                    'evaluation_model': model,
                    'count': len(model_data)
                }
                
                # è¨ˆç®—æ¯å€‹ trait çš„å¹³å‡
                for trait in trait_columns:
                    if trait in model_data.columns:
                        result[f'{trait}_avg'] = model_data[trait].mean()
                    else:
                        result[f'{trait}_avg'] = 0.0
                
                stats_results.append(result)
        
        # è¨ˆç®—æ‰€æœ‰æ¨¡å‹çš„ç¸½å¹³å‡
        if len(prefix_data) > 0:
            result = {
                'prefix': prefix,
                'evaluation_model': 'ALL_AVG',
                'count': len(prefix_data)
            }
            
            for trait in trait_columns:
                if trait in prefix_data.columns:
                    result[f'{trait}_avg'] = prefix_data[trait].mean()
                else:
                    result[f'{trait}_avg'] = 0.0
            
            stats_results.append(result)
    
    # è½‰æ›ç‚º DataFrame
    if stats_results:
        stats_df = pd.DataFrame(stats_results)
        # é‡æ–°æ’åºæ¬„ä½
        base_columns = ['prefix', 'evaluation_model', 'count']
        avg_columns = [f'{trait}_avg' for trait in trait_columns]
        stats_df = stats_df[base_columns + avg_columns]
        return stats_df
    else:
        return pd.DataFrame()

def save_results(df, stats_df, output_dir):
    """å„²å­˜çµæœ"""
    if df is None:
        return
    
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # å„²å­˜åŸå§‹çµæœ
    csv_file = output_path / "comprehensive_trait_averages.csv"
    df.to_csv(csv_file, index=False, encoding='utf-8')
    print(f"ğŸ’¾ CSV æª”æ¡ˆå·²å„²å­˜è‡³: {csv_file}")
    
    # å„²å­˜çµ±è¨ˆçµæœ
    if stats_df is not None and not stats_df.empty:
        stats_csv_file = output_path / "prefix_statistics.csv"
        stats_df.to_csv(stats_csv_file, index=False, encoding='utf-8')
        print(f"ğŸ’¾ çµ±è¨ˆ CSV æª”æ¡ˆå·²å„²å­˜è‡³: {stats_csv_file}")
    
    # å„²å­˜ç‚º Excelï¼ˆå¦‚æœæœ‰ pandas å’Œ openpyxlï¼‰
    try:
        excel_file = output_path / "comprehensive_trait_averages.xlsx"
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='åŸå§‹è³‡æ–™', index=False)
            if stats_df is not None and not stats_df.empty:
                stats_df.to_excel(writer, sheet_name='å‰ç¶´çµ±è¨ˆ', index=False)
        print(f"ğŸ’¾ Excel æª”æ¡ˆå·²å„²å­˜è‡³: {excel_file}")
    except ImportError:
        print("âš ï¸  æœªå®‰è£ openpyxlï¼Œè·³é Excel è¼¸å‡º")
    
    # å„²å­˜ç‚º JSON
    json_file = output_path / "comprehensive_trait_averages.json"
    output_data = {
        'raw_data': df.to_dict('records'),
        'statistics': stats_df.to_dict('records') if stats_df is not None and not stats_df.empty else []
    }
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ JSON æª”æ¡ˆå·²å„²å­˜è‡³: {json_file}")

def print_summary_stats(df, stats_df):
    """åˆ—å°æ‘˜è¦çµ±è¨ˆ"""
    if df is None:
        return
    
    print("\n" + "="*80)
    print("ğŸ“Š æ‘˜è¦çµ±è¨ˆ")
    print("="*80)
    
    # æŒ‰è©•ä¼°æ¨¡å‹åˆ†çµ„
    print("\nğŸ¤– æŒ‰è©•ä¼°æ¨¡å‹åˆ†çµ„:")
    model_counts = df['evaluation_model'].value_counts()
    for model, count in model_counts.items():
        print(f"  {model}: {count} å€‹çµæœ")
    
    # Trait å¹³å‡åˆ†æ•¸
    trait_columns = ['empathetic', 'analytical', 'creative', 'environmental', 'futurist']
    print(f"\nğŸ¯ å„ Trait çš„æ•´é«”å¹³å‡åˆ†æ•¸:")
    for trait in trait_columns:
        if trait in df.columns:
            avg_score = df[trait].mean()
            print(f"  {trait}: {avg_score:.2f}")
    
    # æœ€é«˜åˆ†æ•¸çš„çµ„åˆ
    print(f"\nğŸ† å„ Trait æœ€é«˜åˆ†æ•¸çš„çµ„åˆ:")
    for trait in trait_columns:
        if trait in df.columns:
            max_idx = df[trait].idxmax()
            max_row = df.iloc[max_idx]
            print(f"  {trait}: {max_row[trait]:.2f} ({max_row['persona_combination']} - {max_row['evaluation_model']})")
    
    # é¡¯ç¤ºå‰ç¶´çµ±è¨ˆ
    if stats_df is not None and not stats_df.empty:
        print(f"\nğŸ“‹ å‰ç¶´çµ±è¨ˆ (ç›¸åŒå‰ç¶´çš„å¹³å‡åˆ†æ•¸):")
        print("-" * 80)
        
        # æŒ‰å‰ç¶´åˆ†çµ„é¡¯ç¤º
        for prefix in stats_df['prefix'].unique():
            prefix_data = stats_df[stats_df['prefix'] == prefix]
            print(f"\nğŸ­ {prefix}:")
            
            for _, row in prefix_data.iterrows():
                model = row['evaluation_model']
                count = row['count']
                trait_scores = []
                
                for trait in trait_columns:
                    avg_col = f'{trait}_avg'
                    if avg_col in row:
                        trait_scores.append(f"{trait}:{row[avg_col]:.2f}")
                
                print(f"  {model:10s} (n={count:2d}): " + " | ".join(trait_scores))

def create_comparison_table(df):
    """å»ºç«‹ä¸åŒè©•ä¼°æ¨¡å‹çš„æ¯”è¼ƒè¡¨æ ¼"""
    if df is None:
        return None
    
    # æŒ‰ persona_combination å’Œ evaluation_model é€²è¡Œé€è¦–
    trait_columns = ['empathetic', 'analytical', 'creative', 'environmental', 'futurist']
    
    print(f"\nğŸ“‹ ä¸åŒè©•ä¼°æ¨¡å‹çš„æ¯”è¼ƒ:")
    print("-" * 120)
    
    # å–å¾—æ‰€æœ‰å”¯ä¸€çš„ persona çµ„åˆ
    unique_combos = df['persona_combination'].unique()
    
    for combo in sorted(unique_combos):
        combo_data = df[df['persona_combination'] == combo]
        
        if len(combo_data) > 1:  # åªé¡¯ç¤ºæœ‰å¤šå€‹æ¨¡å‹çµæœçš„çµ„åˆ
            print(f"\nğŸ­ {combo}:")
            for _, row in combo_data.iterrows():
                model = row['evaluation_model']
                scores = [f"{row[trait]:.1f}" for trait in trait_columns if trait in row]
                print(f"  {model:8s}: " + " | ".join(f"{trait}:{score:>6s}" for trait, score in zip(trait_columns, scores)))

def main():
    parser = argparse.ArgumentParser(description="æ•´ç†æ‰€æœ‰ comprehensive è©•ä¼°çµæœçš„ trait_averages")
    parser.add_argument("--result_dir", 
                       default="persona_trait_data/neutral_task/Result",
                       help="çµæœè³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--output_dir",
                       default="analysis_output",
                       help="è¼¸å‡ºè³‡æ–™å¤¾è·¯å¾‘")
    parser.add_argument("--show_comparison", 
                       action="store_true",
                       help="é¡¯ç¤ºä¸åŒè©•ä¼°æ¨¡å‹çš„è©³ç´°æ¯”è¼ƒ")
    
    args = parser.parse_args()
    
    print("ğŸš€ é–‹å§‹åˆ†ææ‰€æœ‰ comprehensive è©•ä¼°çµæœ...")
    
    # è¼‰å…¥æ‰€æœ‰çµæœ
    results = load_all_results(args.result_dir)
    
    if not results:
        print("âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„çµæœæª”æ¡ˆ")
        return
    
    # å»ºç«‹æ‘˜è¦è¡¨æ ¼
    df, stats_df = create_summary_table(results)
    
    # å„²å­˜çµæœ
    save_results(df, stats_df, args.output_dir)
    
    # åˆ—å°æ‘˜è¦çµ±è¨ˆ
    print_summary_stats(df, stats_df)
    
    # é¡¯ç¤ºæ¯”è¼ƒè¡¨æ ¼
    if args.show_comparison:
        create_comparison_table(df)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼å…±è™•ç† {len(results)} å€‹çµæœæª”æ¡ˆ")
    print(f"ğŸ“ è¼¸å‡ºæª”æ¡ˆå„²å­˜åœ¨: {args.output_dir}")

if __name__ == "__main__":
    main()