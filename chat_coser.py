from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from rich.console import Console
from rich.panel import Panel
from rich.prompt import Prompt
from rich.text import Text
from rich.columns import Columns
from rich.align import Align

console = Console()

# Specify local model path
model_path = "./CoSER-Llama-3.1-8B"

# Load local model
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,  # Use float16 for large models to reduce memory usage
    device_map="auto"  # Automatically allocate device (CPU/GPU)
)

# Load local tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path)

def chat_with_model(max_turns=5, max_new_tokens=128):
    # ä½¿ç”¨ rich çš„ Prompt ä¾†å–å¾—ç³»çµ±æç¤º
    console.print(Panel(
        "[bold cyan]æ­¡è¿ä½¿ç”¨è§’è‰²æ‰®æ¼”èŠå¤©æ©Ÿå™¨äºº![/bold cyan]", 
        title="ğŸ­ CoSER Chat", 
        border_style="cyan"
    ))
    
    system_prompt = Prompt.ask("[bold yellow]è«‹è¼¸å…¥è§’è‰²æ‰®æ¼”çš„ç³»çµ±æç¤º[/bold yellow]")

    messages = [{"role": "system", "content": system_prompt}]
    
    console.print(Panel(
        "[green]å°è©±é–‹å§‹ï¼è¼¸å…¥ 'exit' ä¾†çµæŸå°è©±[/green]", 
        border_style="green"
    ))
    
    turn_count = 1
    
    while True:
        # ç¾åŒ–ä½¿ç”¨è€…è¼¸å…¥æç¤º
        user_input = Prompt.ask(f"[bold blue]ğŸ‘¤ æ‚¨ (ç¬¬{turn_count}è¼ª)[/bold blue]").strip()
        
        if user_input.lower() == 'exit':
            console.print(Panel(
                "[bold red]ğŸ­ è§’è‰²: å†è¦‹ï¼æ„Ÿè¬æ‚¨çš„å°è©±ï¼[/bold red]", 
                border_style="red"
            ))
            break
        
        if not user_input:  # è·³éç©ºç™½è¼¸å…¥
            continue
            
        messages.append({"role": "user", "content": user_input})
        
        # é¡¯ç¤ºè™•ç†ä¸­è¨Šæ¯
        with console.status("[bold green]ğŸ¤– è§’è‰²æ­£åœ¨æ€è€ƒä¸­...[/bold green]", spinner="dots"):
            # Process input text
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Prepare model inputs
            model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
            
            # Generate response
            generated_ids = model.generate(
                **model_inputs,
                max_new_tokens=max_new_tokens,
                pad_token_id=tokenizer.eos_token_id,
                do_sample=True,
                temperature=0.7
            )
            
            # Filter out input tokens, keep only the generated part
            generated_ids = [
                output_ids[len(input_ids):] 
                for input_ids, output_ids in zip(model_inputs["input_ids"], generated_ids)
            ]
            
            response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        # ç¾åŒ– QA å°è©±é¡¯ç¤º
        qa_panel = Panel(
            f"[bold blue]ğŸ‘¤ å•é¡Œ:[/bold blue]\n{user_input}\n\n[bold magenta]ğŸ­ å›ç­”:[/bold magenta]\n{response}",
            title=f"ğŸ’¬ å°è©± #{turn_count}",
            border_style="bright_magenta",
            padding=(1, 2)
        )
        
        console.print(qa_panel)
        console.print()  # åŠ å…¥ç©ºè¡Œåˆ†éš”
        
        messages.append({"role": "assistant", "content": response})
        turn_count += 1

if __name__ == "__main__":
    chat_with_model()