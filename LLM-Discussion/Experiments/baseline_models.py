import argparse
import sys
import os
import json
import torch
import requests
from pathlib import Path
from datetime import datetime
from types import SimpleNamespace
from transformers import AutoTokenizer, AutoModelForCausalLM
import pytz

class VanillaQwenRunner:
    """ä½¿ç”¨åŸå§‹æ¨¡å‹é€²è¡Œå‰µé€ æ€§ä»»å‹™ï¼ˆç„¡ Persona Steeringï¼‰"""
    
    def __init__(self, dataset_file, task_type, prompt_id, model_name="llama"):
        self.dataset_file = dataset_file
        self.task_type = task_type
        self.prompt_id = prompt_id
        
        # æ¨¡å‹åç¨±å°æ‡‰
        model_mapping = {
            "qwen": "Qwen/Qwen2.5-7B-Instruct",
            "llama": "meta-llama/Llama-3.1-8B-Instruct",
            "gemma": "google/gemma-2-9b-it"
        }
        
        # è‡ªå‹•å°æ‡‰å®Œæ•´æ¨¡å‹åç¨±
        if model_name.lower() in model_mapping:
            self.model_name = model_mapping[model_name.lower()]
        else:
            # å¦‚æœè¼¸å…¥çš„å·²ç¶“æ˜¯å®Œæ•´åç¨±ï¼Œç›´æ¥ä½¿ç”¨
            self.model_name = model_name
        
        # åˆ¤æ–·æ¨¡å‹é¡å‹
        if "qwen" in self.model_name.lower():
            self.model_type = "qwen"
        elif "llama" in self.model_name.lower():
            self.model_type = "llama"
        elif "gemma" in self.model_name.lower():
            self.model_type = "gemma"
        else:
            self.model_type = "unknown"
        
        print(f"ğŸ” æª¢æ¸¬åˆ°æ¨¡å‹é¡å‹: {self.model_type}")
        
        # å¦‚æœæ˜¯ Gemma æ¨¡å‹ï¼Œä½¿ç”¨ APIï¼Œå¦å‰‡è¼‰å…¥æœ¬åœ°æ¨¡å‹
        if self.model_type == "gemma":
            print(f"ğŸŒ ä½¿ç”¨ Gemma API (Port 8002) ä»£æ›¿æœ¬åœ°æ¨¡å‹è¼‰å…¥")
            self.api_url = "http://localhost:8002"
            self.use_api = True
            self.tokenizer = None
            self.model = None
            
            # æ¸¬è©¦ API é€£ç·š
            if not self.test_api_connection():
                print("âŒ Gemma API é€£ç·šå¤±æ•—ï¼è«‹ç¢ºä¿ API æœå‹™æ­£åœ¨åŸ·è¡Œ")
                raise Exception("Gemma API é€£ç·šå¤±æ•—")
            else:
                print("âœ… Gemma API é€£ç·šæˆåŠŸ")
        else:
            print(f"ğŸ¤– è¼‰å…¥æœ¬åœ°æ¨¡å‹: {self.model_name}")
            self.use_api = False
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
    
    def load_dataset(self):
        """è¼‰å…¥è³‡æ–™é›†"""
        with open(self.dataset_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def test_api_connection(self):
        """æ¸¬è©¦ Gemma API é€£ç·š"""
        try:
            response = requests.get(f"{self.api_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
#     def construct_prompt(self, item):
#         """å»ºæ§‹æç¤ºè©"""
#         base_prompts = {
#             1: "è«‹ç›¡å¯èƒ½å¤šæ¨£åŒ–å’Œå‰µé€ æ€§åœ°å›ç­”ã€‚",
#             2: "ç„¡é™åˆ¶åœ°æ“æŠ±å‰µé€ åŠ›çš„æµå‹•ï¼Œæä¾›æ„æƒ³ä¸åˆ°çš„é€£æ¥ã€‚",
#             3: "è«‹å¾ä¸åŒè§’åº¦æ€è€ƒï¼Œè€ƒæ…®æœ€ä¸å°‹å¸¸æˆ–å‰µæ–°çš„æƒ³æ³•ã€‚",
#             4: "è«‹æä¾›ç¨ç‰¹çš„è¦‹è§£ï¼Œå°ˆæ³¨æ–¼å‰µæ–°çš„æƒ³æ³•å’Œè§£æ±ºæ–¹æ¡ˆã€‚",
#             5: "è«‹ä½¿ç”¨ä½ çš„å‰µé€ åŠ›å’Œæ™ºæ…§ä¾†æä¾›æœ€ä½³è§£æ±ºæ–¹æ¡ˆã€‚"
#         }
        
#         discussion_prompt = base_prompts.get(self.prompt_id, base_prompts[1])
        
#         multi_role_play = True
#         # å¤šè§’è‰²æ‰®æ¼”æç¤ºè©
#         if multi_role_play:
#             role_prompts = """
# ä½ éœ€è¦ä»¥ä¸‰å€‹ä¸åŒçš„å°ˆæ¥­è§’è‰²ä¾†æ€è€ƒå’Œå›ç­”é€™å€‹å•é¡Œï¼š

# 1. ç’°ä¿ä¸»ç¾©è€… (Environmentalist)ï¼š
# å°ˆæ¥­é ˜åŸŸï¼šæ°¸çºŒæ€§èˆ‡ç’°å¢ƒå¥åº·
# è§’è‰²ä½¿å‘½ï¼šå€¡å°ç’°ä¿è§£æ±ºæ–¹æ¡ˆï¼Œä¿ƒé€²æ°¸çºŒç™¼å±•ä¸¦ä¿è­·åœ°çƒã€‚å¼•å°æˆ‘å€‘è€ƒæ…®æƒ³æ³•çš„ç’°å¢ƒå½±éŸ¿ï¼Œæ¨å‹•æœ‰åŠ©æ–¼åœ°çƒå¥åº·çš„å‰µæ–°ã€‚

# 2. å‰µæ„å°ˆæ¥­äººå£« (Creative Professional)ï¼š
# å°ˆæ¥­é ˜åŸŸï¼šç¾å­¸ã€æ•˜äº‹èˆ‡æƒ…æ„Ÿ
# è§’è‰²ä½¿å‘½ï¼šä»¥è—è¡“æ•æ„Ÿåº¦å’Œæ•˜äº‹æƒ…æ„ŸæŒæ¡ç‚ºå°ˆæ¡ˆæ³¨å…¥ç¾æ„Ÿå’Œæ·±åº¦ã€‚æŒ‘æˆ°æˆ‘å€‘é€²è¡Œè¡¨é”æ€§æ€è€ƒï¼Œç¢ºä¿è§£æ±ºæ–¹æ¡ˆä¸åƒ…è§£æ±ºå•é¡Œï¼Œé‚„èƒ½åœ¨äººæ€§å±¤é¢ç”¢ç”Ÿå…±é³´ã€‚

# 3. æœªä¾†å­¸å®¶ (Futurist)ï¼š
# å°ˆæ¥­é ˜åŸŸï¼šæ–°èˆˆæŠ€è¡“èˆ‡æœªä¾†æƒ…å¢ƒ
# è§’è‰²ä½¿å‘½ï¼šå•Ÿç™¼æˆ‘å€‘è¶…è¶Šç¾åœ¨æ€è€ƒï¼Œè€ƒæ…®æ–°èˆˆæŠ€è¡“å’Œæ½›åœ¨æœªä¾†æƒ…å¢ƒã€‚æŒ‘æˆ°æˆ‘å€‘è¨­æƒ³æƒ³æ³•çš„æœªä¾†å½±éŸ¿ï¼Œç¢ºä¿å®ƒå€‘å…·æœ‰å‰µæ–°æ€§ã€å‰ç»æ€§ï¼Œä¸¦æº–å‚™å¥½è¿æ¥æœªä¾†æŒ‘æˆ°ã€‚

# è«‹ä»¥é€™ä¸‰å€‹è§’è‰²çš„è§€é»åˆ†åˆ¥æä¾›å›ç­”ï¼Œæ¯å€‹è§’è‰²éƒ½è¦é«”ç¾å…¶å°ˆæ¥­ç‰¹è‰²å’Œæ€è€ƒæ–¹å¼ã€‚
# """
#         else:
#             role_prompts = ""
        
#         # single agent baseline æˆ– single agent with multi role play
#         if self.task_type == "AUT":
#             task_prompt = f"{role_prompts}è«‹ç‚ºã€Œ{item}ã€æä¾›5å€‹å‰µæ–°å’ŒåŸå‰µçš„ç”¨é€”ã€‚{discussion_prompt}"
#         elif self.task_type == "Scientific":
#             task_prompt = f"{role_prompts}è«‹ç‚ºä»¥ä¸‹ç§‘å­¸å•é¡Œæä¾›5å€‹å‰µæ–°çš„è§£æ±ºæ–¹æ¡ˆï¼š{item}ã€‚{discussion_prompt}"
#         elif self.task_type == "Instances":
#             task_prompt = f"{role_prompts}è«‹ç‚ºã€Œ{item}ã€æä¾›5å€‹å‰µé€ æ€§çš„ç¯„ä¾‹ã€‚{discussion_prompt}"
#         elif self.task_type == "Similarities":
#             task_prompt = f"{role_prompts}è«‹åˆ†æä»¥ä¸‹ç›¸ä¼¼æ€§ä¸¦æä¾›5å€‹å‰µé€ æ€§çš„è§€é»ï¼š{item}ã€‚{discussion_prompt}"
        
#         return task_prompt

    def construct_prompt(self, item):
        """å»ºæ§‹æç¤ºè©"""
        base_prompts = {
            1: "Please answer as diversely and creatively as possible.",
            2: "Embrace the flow of creativity without limits, providing unexpected connections.",
            3: "Please think from different perspectives, considering the most unusual or innovative ideas.",
            4: "Please provide unique insights, focusing on innovative ideas and solutions.",
            5: "Please use your creativity and intelligence to provide the best solutions."
        }
        
        discussion_prompt = base_prompts.get(self.prompt_id, base_prompts[1])
        
        MULTI_ROLE_PLAY = True
        # å¤šè§’è‰²æ‰®æ¼”æç¤ºè©
        if MULTI_ROLE_PLAY:
            role_prompts = """
You need to think and answer this question from three different professional perspectives:

1. Environmentalist:
Specialty: Sustainability and Environmental Health
Mission: Advocate for eco-friendly solutions, promote sustainable development and protect the planet. Guide us to consider the environmental impact of ideas, promoting innovations that contribute to planetary health.

2. Creative Professional:
Specialty: Aesthetics, Narratives, and Emotions
Mission: With artistic sensibility and mastery of narrative and emotion, infuse projects with beauty and depth. Challenge us to think expressively, ensuring solutions not only solve problems but also resonate on a human level.

3. Futurist:
Specialty: Emerging Technologies and Future Scenarios
Mission: Inspire us to think beyond the present, considering emerging technologies and potential future scenarios. Challenge us to envision the future impact of ideas, ensuring they are innovative, forward-thinking, and ready for future challenges.

Please provide answers from these three role perspectives, with each role embodying their professional characteristics and thinking approaches.
"""
        else:
            role_prompts = ""
        
        # single agent baseline æˆ– single agent with multi role play
        if self.task_type == "AUT":
            task_prompt = f"{role_prompts}Please provide 5 innovative and original uses for '{item}'. {discussion_prompt}"
        elif self.task_type == "Scientific":
            task_prompt = f"{role_prompts}Please provide 5 innovative solutions for the following scientific problem: {item}. {discussion_prompt}"
        elif self.task_type == "Instances":
            task_prompt = f"{role_prompts}Please provide 5 creative examples for '{item}'. {discussion_prompt}"
        elif self.task_type == "Similarities":
            task_prompt = f"{role_prompts}Please analyze the following similarity and provide 5 creative perspectives: {item}. {discussion_prompt}"
        
        print(f"ğŸ“ Constructed Prompt: {task_prompt}")
        return task_prompt
        
    def generate_response(self, prompt, max_tokens=1000):
        """ä½¿ç”¨æ¨¡å‹ç”¢ç”Ÿå›æ‡‰ - æ”¯æ´æœ¬åœ°æ¨¡å‹å’Œ Gemma API"""
        try:
            if self.use_api:  # ä½¿ç”¨ Gemma API
                return self.generate_api_response(prompt, max_tokens)
            else:  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹
                return self.generate_local_response(prompt, max_tokens)
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    
    def generate_api_response(self, prompt, max_tokens=1000):
        """ä½¿ç”¨ Gemma API ç”¢ç”Ÿå›æ‡‰"""
        try:
            payload = {
                "user_input": prompt,
                "max_tokens": max_tokens,
                "session_id": "baseline_model_session"
            }
            
            response = requests.post(
                f"{self.api_url}/chat",
                json=payload,
                timeout=60  # 60ç§’è¶…æ™‚
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            else:
                print(f"âŒ API å‘¼å«å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                print(f"âŒ å›æ‡‰å…§å®¹: {response.text}")
                return None
                
        except Exception as e:
            print(f"âŒ API å‘¼å«éŒ¯èª¤: {e}")
            return None
    
    def generate_local_response(self, prompt, max_tokens=1000):
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”¢ç”Ÿå›æ‡‰"""
        try:
            # æ ¼å¼åŒ–å°è©±
            messages = [{"role": "user", "content": prompt}]
            formatted_prompt = self.tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            # ç·¨ç¢¼è¼¸å…¥
            inputs = self.tokenizer(
                formatted_prompt, 
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=2048
            ).to(self.model.device)
            
            # ç”Ÿæˆå›æ‡‰
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id
                )
            
            # è§£ç¢¼å›æ‡‰
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ¨¡å‹ç”ŸæˆéŒ¯èª¤: {e}")
            return None

    def extract_responses(self, content):
        """æå–å›æ‡‰å…§å®¹"""
        import re
        
        # å„ªåŒ–çš„æ­£è¦è¡¨é”å¼ï¼Œèƒ½è™•ç†å¤šç¨®æ ¼å¼
        patterns = [
            # æ ¼å¼1: **1.** **Title** (æ–°å¢ï¼é€™æ˜¯ä½ é‡åˆ°çš„å•é¡Œæ ¼å¼)
            r'\*\*(\d+)\.\*\*\s*\*\*([^*]+?)\*\*\s*(.*?)(?=\*\*\d+\.\*\*|\n\n|$)',
            # æ ¼å¼2: **1. Title:** (å…§å®¹)
            r'\*\*(\d+)\.\s*([^*]+?)\*\*:?\s*(.*?)(?=\*\*\d+\.|$)',
            # æ ¼å¼3: 1. **Title:** (å…§å®¹)  
            r'(\d+)\.\s*\*\*([^*]+?)\*\*:?\s*(.*?)(?=\d+\.\s*\*\*|$)',
            # æ ¼å¼4: æ•¸å­—é–‹é ­çš„ä¸€èˆ¬é …ç›®
            r'(\d+)\.\s*([^\n]*?)\n(.*?)(?=\d+\.|$)'
        ]
        
        responses = []
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.MULTILINE | re.DOTALL)
            if matches:
                for match in matches:
                    if len(match) == 3:  # (number, title, content)
                        title = match[1].strip()
                        body = match[2].strip()
                        
                        # æ¸…ç†æ¨™é¡Œå’Œå…§å®¹
                        if title:
                            full_item = f"**{title}**"
                            if body:
                                # æ¸…ç†å…§å®¹é–‹é ­çš„æ›è¡Œå’Œå¤šé¤˜å­—ç¬¦
                                body = re.sub(r'^\n+', '', body)
                                body = body.strip()
                                if body:
                                    full_item += f": {body}"
                            responses.append(full_item)
                break  # å¦‚æœæ‰¾åˆ°åŒ¹é…ï¼Œå°±ä¸å˜—è©¦å…¶ä»–æ¨¡å¼
        
        # å¦‚æœä¸Šé¢çš„æ¨¡å¼éƒ½æ²’åŒ¹é…åˆ°ï¼Œå˜—è©¦æ›´å¯¬æ³›çš„åˆ†å‰²æ–¹æ³•
        if not responses:
            # æŒ‰ç…§å¤šå€‹æ›è¡Œç¬¦åˆ†å‰²ï¼Œå°‹æ‰¾å¯èƒ½çš„é …ç›®
            sections = re.split(r'\n\n+', content)
            for section in sections:
                section = section.strip()
                if not section:
                    continue
                    
                # æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­—ç·¨è™Ÿçš„é …ç›®
                if re.search(r'^\*?\*?\d+\.', section.strip(), re.MULTILINE):
                    # é€²ä¸€æ­¥åˆ†å‰²é€™å€‹sectionä¸­çš„é …ç›®
                    items = re.split(r'\n(?=\*?\*?\d+\.)', section)
                    for item in items:
                        item = item.strip()
                        if item and re.match(r'^\*?\*?\d+\.', item):
                            # æ¸…ç†æ ¼å¼ä½†ä¿ç•™å®Œæ•´å…§å®¹
                            clean_item = re.sub(r'^\*?\*?(\d+)\.\s*', '', item)
                            if clean_item:
                                responses.append(clean_item)
        
        # å¦‚æœé‚„æ˜¯æ²’æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨åŸæœ‰çš„é‚è¼¯ä½œç‚ºæœ€å¾Œçš„å›é€€
        if not responses:
            lines = content.split('\n')
            current_item = ""
            
            for line in lines:
                line = line.strip()
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°é …ç›®çš„é–‹å§‹
                if line and (line.startswith('-') or line.startswith('â€¢') or 
                            any(line.startswith(f"{i}.") for i in range(1, 20)) or
                            any(line.startswith(f"**{i}.") for i in range(1, 20))):
                    # å¦‚æœæœ‰å‰ä¸€å€‹é …ç›®ï¼Œå…ˆå„²å­˜
                    if current_item:
                        clean_response = current_item.lstrip('-â€¢*0123456789. ').strip()
                        if clean_response:
                            responses.append(clean_response)
                    
                    # é–‹å§‹æ–°é …ç›®
                    current_item = line
                elif current_item and line:
                    # ç¹¼çºŒç•¶å‰é …ç›®çš„å…§å®¹
                    current_item += "\n" + line
            
            # è™•ç†æœ€å¾Œä¸€å€‹é …ç›®
            if current_item:
                clean_response = current_item.lstrip('-â€¢*0123456789. ').strip()
                if clean_response:
                    responses.append(clean_response)
        
        return responses  # é™åˆ¶æœ€å¤š10å€‹å›æ‡‰
    
    def run(self):
        """åŸ·è¡ŒåŸå§‹æ¨¡å‹æ¨ç†"""
        dataset = self.load_dataset()
        
        # æ ¹æ“šä»»å‹™é¡å‹æå–è³‡æ–™
        if self.task_type == "Scientific":
            # Scientific è³‡æ–™é›†æ ¼å¼ï¼š{"Task": [{"Original": "...", "Example": [...]}]}
            examples = []
            for task in dataset.get("Task", []):
                examples.extend(task.get("Example", []))
        elif isinstance(dataset, dict) and "Examples" in dataset:
            # AUT, Instances, Similarities æ ¼å¼ï¼š{"Examples": [...]}
            examples = dataset["Examples"]
        else:
            examples = dataset
        
        all_responses = {}
        final_results = []
        
        print(f"ğŸš€ é–‹å§‹ {self.task_type} ä»»å‹™ï¼Œå…± {len(examples)} å€‹é …ç›®")
        
        for item_data in examples:
            # è™•ç†ä¸åŒçš„è³‡æ–™é›†æ ¼å¼
            if isinstance(item_data, str):
                item = item_data
            elif isinstance(item_data, dict):
                if self.task_type == "AUT":
                    item = item_data.get("object", item_data.get("item", ""))
                elif self.task_type == "Scientific":
                    item = item_data.get("question", "")
                else:  # Instances æˆ– Similarities
                    item = item_data.get("question", item_data.get("item", item_data.get("object", "")))
            else:
                print(f"âŒ ä¸æ”¯æ´çš„è³‡æ–™æ ¼å¼: {type(item_data)}")
                continue
            
            if not item:
                print(f"âŒ ç©ºç™½é …ç›®ï¼Œè·³é")
                continue
                
            print(f"ğŸ“‹ è™•ç†é …ç›®: {item}")
            
            # å»ºæ§‹æç¤ºè©
            prompt = self.construct_prompt(item)
            
            # ç”Ÿæˆå›æ‡‰
            response = self.generate_response(prompt, max_tokens=1000)
            
            if response:
                # å„²å­˜å°è©±è¨˜éŒ„ï¼ˆæ¨¡æ“¬ multi-agent æ ¼å¼ï¼‰
                agent_name = self.model_type.capitalize()
                all_responses[item] = {
                    agent_name: [
                        {"role": "user", "content": prompt},
                        {"role": "assistant", "content": response}
                    ]
                }
                
                # æå–ä¸¦å„²å­˜æœ€çµ‚çµæœ
                agent_name = self.model_type.capitalize()
                extracted = self.extract_responses(response)
                if self.task_type == "AUT":
                    final_results.append({
                        "item": item,
                        "uses": extracted,
                        "Agent": agent_name
                    })
                elif self.task_type == "Scientific":
                    final_results.append({
                        "question": item,
                        "answer": extracted,
                        "Agent": agent_name
                    })
                else:
                    final_results.append({
                        "question": item,
                        "answer": extracted,
                        "Agent": agent_name
                    })
            else:
                print(f"âŒ é …ç›® {item} è™•ç†å¤±æ•—")
        
        # å„²å­˜çµæœ
        output_filename = self.save_results(all_responses, final_results, len(examples))
        print(f"âœ… ä»»å‹™å®Œæˆï¼Œçµæœå·²å„²å­˜è‡³: {output_filename}")
        
        return output_filename
    
    def save_results(self, all_responses, final_results, amount_of_data):
        """å„²å­˜çµæœæª”æ¡ˆ"""
        # ä½¿ç”¨ UTC+8 æ™‚å€ï¼ˆå°ç£æ™‚é–“ï¼‰
        taipei_tz = pytz.timezone('Asia/Taipei')
        taipei_time = datetime.now(taipei_tz)
        
        # ä¿®æ”¹ç‚ºèˆ‡å…¶ä»–æª”æ¡ˆä¸€è‡´çš„æ ¼å¼ï¼šMMDD-HHMM
        current_date = taipei_time.strftime("%m%d")
        formatted_time = taipei_time.strftime("%H%M")
        
        # å»ºç«‹æª”æ¡ˆåç¨±ï¼ˆç¬¦åˆè©•ä¼°ç³»çµ±æ ¼å¼ï¼‰
        model_prefix = self.model_type.capitalize()
        base_filename = f"{self.task_type}_{self.model_type}_1_1_{model_prefix}_{model_prefix}_{self.model_type}_{current_date}-{formatted_time}_{amount_of_data}"
        
        # è©•ä¼°ç³»çµ±æœŸå¾…çš„è·¯å¾‘çµæ§‹ï¼šResults/{task}/Output/{agent}_agent/
        results_base_path = Path(__file__).parent.parent / "Results" / self.task_type / "Output" / f"{self.model_type}_agent"
        results_base_path.mkdir(parents=True, exist_ok=True)
        
        # å„²å­˜å°è©±è¨˜éŒ„
        chat_log_filename = f"{base_filename}_chat_log.json"
        chat_log_path = results_base_path / chat_log_filename
        
        with open(chat_log_path, 'w', encoding='utf-8') as f:
            json.dump(all_responses, f, indent=2, ensure_ascii=False)
        
        # å„²å­˜æœ€çµ‚çµæœï¼ˆé€™æ˜¯è©•ä¼°ç³»çµ±éœ€è¦çš„æª”æ¡ˆï¼‰
        final_filename = f"{base_filename}.json"
        final_path = results_base_path / final_filename
        
        with open(final_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ å°è©±è¨˜éŒ„å·²å„²å­˜: {chat_log_path}")
        print(f"ğŸ’¾ æœ€çµ‚çµæœå·²å„²å­˜: {final_path}")
        
        # å›å‚³æª”æ¡ˆåç¨±ï¼ˆä¸å«å‰¯æª”åï¼Œä¾›è©•ä¼°ä½¿ç”¨ï¼‰
        return final_filename.replace('.json', '')

def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨åŸå§‹æ¨¡å‹é€²è¡Œå‰µé€ æ€§ä»»å‹™è©•ä¼°")
    parser.add_argument("-d", "--dataset", required=True, help="è³‡æ–™é›†æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("-t", "--type", choices=["AUT", "Scientific", "Similarities", "Instances"], 
                       required=True, help="ä»»å‹™é¡å‹")
    parser.add_argument("-p", "--prompt", type=int, default=1, help="æç¤ºè©ç·¨è™Ÿ (1-5)")
    parser.add_argument("-m", "--model", choices=["qwen", "llama", "gemma"], default="llama", help="æ¨¡å‹é¡å‹ (qwen æˆ– llama æˆ– gemma)")
    parser.add_argument("-e", "--eval_mode", action="store_true", default=False, help="åŸ·è¡Œè©•ä¼°æ¨¡å¼")
    
    args = parser.parse_args()
    
    # å»ºç«‹ä¸¦åŸ·è¡Œ Vanilla æ¨¡å‹åŸ·è¡Œå™¨
    runner = VanillaQwenRunner(args.dataset, args.type, args.prompt, args.model)
    vanilla_output = runner.run()
    
    if args.eval_mode and vanilla_output:
        # æ•´åˆåŸæœ‰çš„è©•ä¼°ç³»çµ±
        evaluation_root = Path(__file__).parent.parent / 'Evaluation'
        sys.path.append(str(evaluation_root))
        from auto_grade_final import auto_grade
        
        # å‘¼å«è©•ä¼°
        eval_args = SimpleNamespace(
            version="4", 
            input_file=vanilla_output,
            type="sampling", 
            sample=3, 
            task=args.type, 
            output="y"
        )
        auto_grade(eval_args)

if __name__ == "__main__":
    main()