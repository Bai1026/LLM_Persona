#!/usr/bin/env python3
"""
Gemma API æœå‹™
æä¾› HTTP API ä»‹é¢è®“å¤–éƒ¨ç¨‹å¼å‘¼å« Gemma æ¨¡å‹
"""

from flask import Flask, request, jsonify
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import logging
import os
import threading
import time
from datetime import datetime

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gemma_api.log'),
        logging.StreamHandler()
    ]
)

app = Flask(__name__)

class GemmaAPIService:
    """Gemma API æœå‹™é¡åˆ¥"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.conversation_history = {}  # å„²å­˜å°è©±æ­·å²
        self.model_loaded = False
        
    def load_model(self):
        """è¼‰å…¥ Gemma æ¨¡å‹"""
        model_name = "google/gemma-3-4b-it"
        logging.info(f"ğŸ”„ è¼‰å…¥æ¨¡å‹: {model_name}")
        logging.info(f"ğŸ“± ä½¿ç”¨è¨­å‚™: {self.device}")
        
        try:
            # è¼‰å…¥åˆ†è©å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # è¼‰å…¥æ¨¡å‹
            if self.device == "cuda":
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True
                )
            
            self.model_loaded = True
            logging.info("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")
            return True
            
        except Exception as e:
            logging.error(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False
    
    def generate_response(self, user_input: str, max_tokens: int = 1000, session_id: str = "default"):
        """ç”Ÿæˆå›æ‡‰"""
        if not self.model_loaded:
            return None, "æ¨¡å‹å°šæœªè¼‰å…¥"
        
        try:
            # æ§‹é€ å°è©±æ ¼å¼
            conversation = [
                {"role": "user", "content": user_input}
            ]
            
            # æ‡‰ç”¨èŠå¤©æ¨¡æ¿
            prompt = self.tokenizer.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç·¨ç¢¼
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            )
            
            # ç§»å‹•åˆ°æ­£ç¢ºè¨­å‚™
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # ç”Ÿæˆå›æ‡‰
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # è§£ç¢¼å›æ‡‰
            full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # æå–æ¨¡å‹å›æ‡‰éƒ¨åˆ†
            response = full_response[len(prompt):].strip()
            
            # å„²å­˜å°è©±æ­·å²
            if session_id not in self.conversation_history:
                self.conversation_history[session_id] = []
            
            self.conversation_history[session_id].append({
                "user": user_input,
                "assistant": response,
                "timestamp": datetime.now().isoformat()
            })
            
            return response, None
            
        except Exception as e:
            error_msg = f"ç”Ÿæˆå›æ‡‰å¤±æ•—: {str(e)}"
            logging.error(error_msg)
            return None, error_msg
    
    def reset_conversation(self, session_id: str = "default"):
        """é‡è¨­å°è©±æ­·å²"""
        if session_id in self.conversation_history:
            del self.conversation_history[session_id]
        return True

# å»ºç«‹å…¨åŸŸæœå‹™å¯¦ä¾‹
gemma_service = GemmaAPIService()

@app.route('/status', methods=['GET'])
def status():
    """æª¢æŸ¥æœå‹™ç‹€æ…‹"""
    return jsonify({
        "status": "running",
        "model_loaded": gemma_service.model_loaded,
        "device": gemma_service.device,
        "timestamp": datetime.now().isoformat()
    })

@app.route('/chat', methods=['POST'])
def chat():
    """èŠå¤©ç«¯é»"""
    try:
        data = request.get_json()
        
        # æª¢æŸ¥å¿…è¦åƒæ•¸
        if not data or 'user_input' not in data:
            return jsonify({"error": "ç¼ºå°‘ user_input åƒæ•¸"}), 400
        
        user_input = data['user_input']
        max_tokens = data.get('max_tokens', 1000)
        session_id = data.get('session_id', 'default')
        
        # ç”Ÿæˆå›æ‡‰
        response, error = gemma_service.generate_response(user_input, max_tokens, session_id)
        
        if error:
            return jsonify({"error": error}), 500
        
        return jsonify({
            "response": response,
            "session_id": session_id,
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"èŠå¤©ç«¯é»éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/reset', methods=['POST'])
def reset():
    """é‡è¨­å°è©±æ­·å²"""
    try:
        data = request.get_json() or {}
        session_id = data.get('session_id', 'default')
        
        gemma_service.reset_conversation(session_id)
        
        return jsonify({
            "message": f"æœƒè©± {session_id} å·²é‡è¨­",
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"é‡è¨­ç«¯é»éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/history', methods=['GET'])
def history():
    """ç²å–å°è©±æ­·å²"""
    try:
        session_id = request.args.get('session_id', 'default')
        
        history = gemma_service.conversation_history.get(session_id, [])
        
        return jsonify({
            "session_id": session_id,
            "history": history,
            "count": len(history),
            "timestamp": datetime.now().isoformat()
        })
        
    except Exception as e:
        logging.error(f"æ­·å²ç«¯é»éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health():
    """å¥åº·æª¢æŸ¥"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    })

def load_model_async():
    """ç•°æ­¥è¼‰å…¥æ¨¡å‹"""
    logging.info("ğŸš€ é–‹å§‹è¼‰å…¥ Gemma æ¨¡å‹...")
    success = gemma_service.load_model()
    if success:
        logging.info("âœ… Gemma æ¨¡å‹è¼‰å…¥å®Œæˆï¼ŒAPI æœå‹™å·²å°±ç·’")
    else:
        logging.error("âŒ Gemma æ¨¡å‹è¼‰å…¥å¤±æ•—")

def main():
    """ä¸»å‡½å¼"""
    print("ğŸ¤– Gemma API æœå‹™å•Ÿå‹•ä¸­...")
    
    # åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­è¼‰å…¥æ¨¡å‹
    model_thread = threading.Thread(target=load_model_async, daemon=True)
    model_thread.start()
    
    # è¨­å®š Flask æ‡‰ç”¨
    port = int(os.environ.get('GEMMA_API_PORT', 8002))
    host = os.environ.get('GEMMA_API_HOST', '0.0.0.0')
    
    print(f"ğŸŒ API æœå‹™å°‡åœ¨ http://{host}:{port} å•Ÿå‹•")
    print("ğŸ“¡ å¯ç”¨ç«¯é»:")
    print("  - GET  /status   - æª¢æŸ¥æœå‹™ç‹€æ…‹")
    print("  - POST /chat     - èŠå¤©å°è©±")
    print("  - POST /reset    - é‡è¨­å°è©±")
    print("  - GET  /history  - ç²å–å°è©±æ­·å²")
    print("  - GET  /health   - å¥åº·æª¢æŸ¥")
    print("\nç­‰å¾…æ¨¡å‹è¼‰å…¥å®Œæˆ...")
    
    # å•Ÿå‹• Flask æ‡‰ç”¨
    app.run(
        host=host,
        port=port,
        debug=False,
        threaded=True
    )

if __name__ == "__main__":
    main()
