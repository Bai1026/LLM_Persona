#!/usr/bin/env python3
"""
Token è¨ˆç®—å™¨ - è¨ˆç®— JSON å°è©±æª”æ¡ˆä¸­çš„ input å’Œ output tokens
æ”¯æ´å…©ç¨®æ ¼å¼ï¼š
1. å–®ç´”å°è©±æ ¼å¼ (user = input, assistant = output)
2. Discussion æ ¼å¼ (è¤‡é›œçš„å¤šä»£ç†è¨è«–æ ¼å¼)
"""

import json
import argparse
import tiktoken
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class TokenStats:
    """Token çµ±è¨ˆçµæœ"""
    input_tokens: int = 0
    output_tokens: int = 0
    total_tokens: int = 0
    
    def add(self, other: 'TokenStats'):
        """åˆä½µçµ±è¨ˆ"""
        self.input_tokens += other.input_tokens
        self.output_tokens += other.output_tokens
        self.total_tokens += other.total_tokens

class TokenCounter:
    """Token è¨ˆç®—å™¨"""
    
    def __init__(self, model_name: str = "gpt-4"):
        """åˆå§‹åŒ– tokenizer"""
        try:
            self.encoding = tiktoken.encoding_for_model(model_name)
        except KeyError:
            # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨ï¼Œä½¿ç”¨ cl100k_base (GPT-4 çš„ç·¨ç¢¼)
            self.encoding = tiktoken.get_encoding("cl100k_base")
        
    def count_tokens(self, text: str) -> int:
        """è¨ˆç®—æ–‡å­—çš„ token æ•¸é‡"""
        if not text:
            return 0
        return len(self.encoding.encode(text))
    
    def count_message_tokens(self, messages: List[Dict[str, str]]) -> TokenStats:
        """è¨ˆç®—ä¸€çµ„è¨Šæ¯çš„ tokens"""
        stats = TokenStats()
        
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            token_count = self.count_tokens(content)
            
            if role == "user":
                stats.input_tokens += token_count
            elif role == "assistant":
                stats.output_tokens += token_count
            
        stats.total_tokens = stats.input_tokens + stats.output_tokens
        return stats
    
    def analyze_simple_format(self, data: List[Dict]) -> TokenStats:
        """åˆ†æç°¡å–®å°è©±æ ¼å¼"""
        total_stats = TokenStats()
        
        print("ğŸ“Š åˆ†æç°¡å–®å°è©±æ ¼å¼...")
        
        for i, conversation in enumerate(data):
            if "PersonaAPI" in conversation:
                messages = conversation["PersonaAPI"]
                stats = self.count_message_tokens(messages)
                
                print(f"  å°è©± {i+1}: Input={stats.input_tokens}, Output={stats.output_tokens}")
                total_stats.add(stats)
        
        return total_stats
    
    def analyze_discussion_format(self, data: Dict) -> TokenStats:
        """åˆ†æ discussion æ ¼å¼"""
        total_stats = TokenStats()
        
        print("ğŸ“Š åˆ†æ Discussion æ ¼å¼...")
        
        for question, agents in data.items():
            print(f"\nâ“ å•é¡Œ: {question[:100]}...")
            
            for agent_name, conversations in agents.items():
                print(f"  ğŸ¤– ä»£ç†: {agent_name}")
                
                stats = self.count_message_tokens(conversations)
                print(f"     Input={stats.input_tokens}, Output={stats.output_tokens}")
                
                total_stats.add(stats)
        
        return total_stats
    
    def detect_format(self, data: Any) -> str:
        """æª¢æ¸¬æª”æ¡ˆæ ¼å¼"""
        if isinstance(data, list):
            return "simple"
        elif isinstance(data, dict):
            # æª¢æŸ¥æ˜¯å¦ç‚º discussion æ ¼å¼
            for key, value in data.items():
                if isinstance(value, dict):
                    for agent_name, conversations in value.items():
                        if isinstance(conversations, list) and conversations:
                            if "role" in conversations[0]:
                                return "discussion"
            return "unknown"
        else:
            return "unknown"
    
    def analyze_file(self, file_path: str) -> TokenStats:
        """åˆ†ææª”æ¡ˆä¸¦è¿”å›çµ±è¨ˆçµæœ"""
        print(f"ğŸ“ è®€å–æª”æ¡ˆ: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆå¤±æ•—: {e}")
            return TokenStats()
        
        format_type = self.detect_format(data)
        print(f"ğŸ” æª¢æ¸¬åˆ°æ ¼å¼: {format_type}")
        
        if format_type == "simple":
            return self.analyze_simple_format(data)
        elif format_type == "discussion":
            return self.analyze_discussion_format(data)
        else:
            print("âŒ æœªçŸ¥çš„æª”æ¡ˆæ ¼å¼")
            return TokenStats()

def print_results(stats: TokenStats, file_name: str):
    """å°å‡ºçµæœ"""
    print(f"\n" + "="*60)
    print(f"ğŸ“ˆ æª”æ¡ˆ {file_name} çš„ Token çµ±è¨ˆçµæœ")
    print("="*60)
    print(f"ğŸ“¥ Input Tokens (user):       {stats.input_tokens:,}")
    print(f"ğŸ“¤ Output Tokens (assistant): {stats.output_tokens:,}")
    print(f"ğŸ“Š Total Tokens:              {stats.total_tokens:,}")
    print("="*60)
    
    # è¨ˆç®—æ¯”ä¾‹
    if stats.total_tokens > 0:
        input_ratio = (stats.input_tokens / stats.total_tokens) * 100
        output_ratio = (stats.output_tokens / stats.total_tokens) * 100
        print(f"ğŸ“Š Input æ¯”ä¾‹:   {input_ratio:.1f}%")
        print(f"ğŸ“Š Output æ¯”ä¾‹:  {output_ratio:.1f}%")

def main():
    parser = argparse.ArgumentParser(description="è¨ˆç®—å°è©±æª”æ¡ˆä¸­çš„ input/output tokens")
    parser.add_argument("file_path", help="JSON æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--model", default="gpt-4", help="ä½¿ç”¨çš„æ¨¡å‹åç¨± (é è¨­: gpt-4)")
    parser.add_argument("--output", "-o", help="è¼¸å‡ºçµæœåˆ°æª”æ¡ˆ")
    
    args = parser.parse_args()
    
    # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not Path(args.file_path).exists():
        print(f"âŒ æª”æ¡ˆä¸å­˜åœ¨: {args.file_path}")
        return
    
    # å»ºç«‹ token è¨ˆç®—å™¨
    counter = TokenCounter(args.model)
    
    # åˆ†ææª”æ¡ˆ
    stats = counter.analyze_file(args.file_path)
    
    # å°å‡ºçµæœ
    file_name = Path(args.file_path).name
    print_results(stats, file_name)
    
    # è¼¸å‡ºåˆ°æª”æ¡ˆ
    if args.output:
        try:
            result = {
                "file": file_name,
                "model": args.model,
                "input_tokens": stats.input_tokens,
                "output_tokens": stats.output_tokens,
                "total_tokens": stats.total_tokens,
                "input_ratio": (stats.input_tokens / stats.total_tokens * 100) if stats.total_tokens > 0 else 0,
                "output_ratio": (stats.output_tokens / stats.total_tokens * 100) if stats.total_tokens > 0 else 0
            }
            
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ°: {args.output}")
        except Exception as e:
            print(f"âŒ å„²å­˜å¤±æ•—: {e}")

if __name__ == "__main__":
    main()
