#!/usr/bin/env python3
"""
æ‰¹æ¬¡ Token è¨ˆç®—å™¨ - è™•ç†å¤šå€‹æª”æ¡ˆ
"""

import json
import argparse
import os
import pandas as pd
from pathlib import Path
from token_counter import TokenCounter
from typing import List, Dict

def find_json_files(directory: str, pattern: str = "*.json") -> List[str]:
    """å°‹æ‰¾æŒ‡å®šç›®éŒ„ä¸‹çš„ JSON æª”æ¡ˆ"""
    path = Path(directory)
    if not path.exists():
        print(f"âŒ ç›®éŒ„ä¸å­˜åœ¨: {directory}")
        return []
    
    json_files = list(path.glob(pattern))
    return [str(f) for f in json_files]

def process_multiple_files(file_paths: List[str], model: str = "gpt-4") -> List[Dict]:
    """è™•ç†å¤šå€‹æª”æ¡ˆ"""
    counter = TokenCounter(model)
    results = []
    
    print(f"ğŸ“ æ‰¾åˆ° {len(file_paths)} å€‹æª”æ¡ˆ")
    print("="*60)
    
    for i, file_path in enumerate(file_paths, 1):
        print(f"ğŸ“„ è™•ç†æª”æ¡ˆ {i}/{len(file_paths)}: {Path(file_path).name}")
        
        try:
            stats = counter.analyze_file(file_path)
            
            result = {
                "æª”æ¡ˆåç¨±": Path(file_path).name,
                "æª”æ¡ˆè·¯å¾‘": file_path,
                "Input_Tokens": stats.input_tokens,
                "Output_Tokens": stats.output_tokens,
                "Total_Tokens": stats.total_tokens,
                "Input_æ¯”ä¾‹(%)": round((stats.input_tokens / stats.total_tokens * 100) if stats.total_tokens > 0 else 0, 2),
                "Output_æ¯”ä¾‹(%)": round((stats.output_tokens / stats.total_tokens * 100) if stats.total_tokens > 0 else 0, 2)
            }
            
            results.append(result)
            print(f"   âœ… Input: {stats.input_tokens:,}, Output: {stats.output_tokens:,}, Total: {stats.total_tokens:,}")
            
        except Exception as e:
            print(f"   âŒ è™•ç†å¤±æ•—: {e}")
            result = {
                "æª”æ¡ˆåç¨±": Path(file_path).name,
                "æª”æ¡ˆè·¯å¾‘": file_path,
                "Input_Tokens": 0,
                "Output_Tokens": 0,
                "Total_Tokens": 0,
                "Input_æ¯”ä¾‹(%)": 0,
                "Output_æ¯”ä¾‹(%)": 0,
                "éŒ¯èª¤": str(e)
            }
            results.append(result)
        
        print()
    
    return results

def print_summary(results: List[Dict]):
    """å°å‡ºæ‘˜è¦çµ±è¨ˆ"""
    if not results:
        print("âŒ æ²’æœ‰çµæœå¯ä»¥é¡¯ç¤º")
        return
    
    total_input = sum(r["Input_Tokens"] for r in results)
    total_output = sum(r["Output_Tokens"] for r in results)
    total_all = sum(r["Total_Tokens"] for r in results)
    
    print("="*80)
    print("ğŸ“ˆ æ‰¹æ¬¡è™•ç†æ‘˜è¦çµ±è¨ˆ")
    print("="*80)
    print(f"ğŸ“ è™•ç†æª”æ¡ˆæ•¸é‡:     {len(results)}")
    print(f"ğŸ“¥ ç¸½ Input Tokens:   {total_input:,}")
    print(f"ğŸ“¤ ç¸½ Output Tokens:  {total_output:,}")
    print(f"ğŸ“Š ç¸½è¨ˆ Tokens:       {total_all:,}")
    
    if total_all > 0:
        print(f"ğŸ“Š å¹³å‡ Input æ¯”ä¾‹:   {(total_input / total_all * 100):.1f}%")
        print(f"ğŸ“Š å¹³å‡ Output æ¯”ä¾‹:  {(total_output / total_all * 100):.1f}%")
    
    print("="*80)

def save_to_csv(results: List[Dict], output_file: str):
    """å„²å­˜çµæœåˆ° CSV"""
    try:
        df = pd.DataFrame(results)
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ° CSV: {output_file}")
    except Exception as e:
        print(f"âŒ å„²å­˜ CSV å¤±æ•—: {e}")

def save_to_json(results: List[Dict], output_file: str):
    """å„²å­˜çµæœåˆ° JSON"""
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ çµæœå·²å„²å­˜åˆ° JSON: {output_file}")
    except Exception as e:
        print(f"âŒ å„²å­˜ JSON å¤±æ•—: {e}")

def main():
    parser = argparse.ArgumentParser(description="æ‰¹æ¬¡è¨ˆç®—å¤šå€‹å°è©±æª”æ¡ˆçš„ tokens")
    parser.add_argument("directory", help="åŒ…å« JSON æª”æ¡ˆçš„ç›®éŒ„")
    parser.add_argument("--pattern", default="*.json", help="æª”æ¡ˆæœå°‹æ¨¡å¼ (é è¨­: *.json)")
    parser.add_argument("--model", default="gpt-4", help="ä½¿ç”¨çš„æ¨¡å‹åç¨± (é è¨­: gpt-4)")
    parser.add_argument("--output-csv", help="è¼¸å‡º CSV æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--output-json", help="è¼¸å‡º JSON æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--recursive", "-r", action="store_true", help="éè¿´æœå°‹å­ç›®éŒ„")
    
    args = parser.parse_args()
    
    # å°‹æ‰¾æª”æ¡ˆ
    if args.recursive:
        pattern = f"**/{args.pattern}"
    else:
        pattern = args.pattern
    
    file_paths = find_json_files(args.directory, pattern)
    
    if not file_paths:
        print(f"âŒ åœ¨ç›®éŒ„ {args.directory} ä¸­æ‰¾ä¸åˆ°ç¬¦åˆ {pattern} çš„æª”æ¡ˆ")
        return
    
    # è™•ç†æª”æ¡ˆ
    results = process_multiple_files(file_paths, args.model)
    
    # å°å‡ºæ‘˜è¦
    print_summary(results)
    
    # å„²å­˜çµæœ
    if args.output_csv:
        save_to_csv(results, args.output_csv)
    
    if args.output_json:
        save_to_json(results, args.output_json)

if __name__ == "__main__":
    main()
