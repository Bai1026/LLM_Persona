from flask import Flask, request, jsonify
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import argparse
from typing import List, Dict
import gc

app = Flask(__name__)

class PureModelChatbot:
    """ç´”ç²¹çš„ Qwen/Llama èŠå¤©æ©Ÿå™¨äººï¼Œä¸ä½¿ç”¨ vector"""
    
    def __init__(self, model_name: str, device: str = "auto"):
        self.model_name = model_name
        self.device = device
        self.conversation_history = []
        
        print(f"ğŸš€ æ­£åœ¨è¼‰å…¥æ¨¡å‹: {model_name}")
        
        # è¼‰å…¥ tokenizer å’Œæ¨¡å‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map=device,
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ")
        print(f"ğŸ“± è£ç½®: {self.model.device}")
    
    def generate_response(self, user_input: str, max_tokens: int = 1000, temperature: float = 0.7) -> str:
        """ç”¢ç”Ÿå›æ‡‰"""
        try:
            # å»ºæ§‹å°è©±æ ¼å¼
            messages = self.conversation_history + [{"role": "user", "content": user_input}]
            
            # ä½¿ç”¨ tokenizer çš„ chat template
            if hasattr(self.tokenizer, 'apply_chat_template'):
                formatted_input = self.tokenizer.apply_chat_template(
                    messages, 
                    tokenize=False, 
                    add_generation_prompt=True
                )
            else:
                # å¦‚æœæ²’æœ‰ chat templateï¼Œä½¿ç”¨ç°¡å–®æ ¼å¼
                formatted_input = self._format_conversation(messages)
            
            # Tokenize
            inputs = self.tokenizer(
                formatted_input, 
                return_tensors="pt", 
                padding=True, 
                truncation=True,
                max_length=2048
            )
            
            # ç§»åˆ°æ­£ç¢ºçš„è£ç½®
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
            
            # ç”¢ç”Ÿå›æ‡‰
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # è§£ç¢¼å›æ‡‰
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:], 
                skip_special_tokens=True
            ).strip()
            
            # æ›´æ–°å°è©±æ­·å²
            self.conversation_history.append({"role": "user", "content": user_input})
            self.conversation_history.append({"role": "assistant", "content": response})
            
            # é™åˆ¶å°è©±æ­·å²é•·åº¦
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[-20:]

            return response
            
        except Exception as e:
            print(f"âŒ ç”¢ç”Ÿå›æ‡‰æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return f"æŠ±æ­‰ï¼Œè™•ç†æ‚¨çš„è¨Šæ¯æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
    
    def _format_conversation(self, messages: List[Dict[str, str]]) -> str:
        """æ ¼å¼åŒ–å°è©±ï¼ˆç•¶æ²’æœ‰ chat template æ™‚ä½¿ç”¨ï¼‰"""
        formatted = ""
        for msg in messages:
            if msg["role"] == "user":
                formatted += f"User: {msg['content']}\n"
            elif msg["role"] == "assistant":
                formatted += f"Assistant: {msg['content']}\n"
        formatted += "Assistant: "
        return formatted
    
    def reset_conversation(self):
        """é‡è¨­å°è©±æ­·å²"""
        self.conversation_history = []
        print("ğŸ”„ å°è©±æ­·å²å·²æ¸…é™¤")
    
    def get_status(self) -> Dict:
        """å–å¾—ç‹€æ…‹è³‡è¨Š"""
        return {
            "model_name": self.model_name,
            "device": str(self.model.device),
            "conversation_length": len(self.conversation_history),
            "memory_allocated": torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        }

# å…¨åŸŸè®Šæ•¸
chatbot = None

@app.route('/chat', methods=['POST'])
def chat_api():
    """èŠå¤© API ç«¯é»"""
    try:
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({"error": "ç¼ºå°‘ message åƒæ•¸"}), 400
        
        user_input = data['message']
        max_tokens = data.get('max_tokens', 512)
        temperature = data.get('temperature', 0.7)
        
        print(f"ğŸ§‘ User: {user_input}")
        response = chatbot.generate_response(user_input, max_tokens, temperature)
        print(f"ğŸ¤– Assistant: {response}")
        
        return jsonify({
            "message": user_input,
            "response": response,
            "status": "success"
        })
        
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/reset', methods=['POST'])
def reset_conversation():
    """é‡è¨­å°è©±æ­·å²"""
    try:
        chatbot.reset_conversation()
        return jsonify({"status": "conversation reset"})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/status', methods=['GET'])
def get_status():
    """å–å¾—æ¨¡å‹ç‹€æ…‹"""
    try:
        status = chatbot.get_status()
        return jsonify(status)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æª¢æŸ¥"""
    return jsonify({"status": "healthy", "model_loaded": chatbot is not None})

def main():
    global chatbot
    
    parser = argparse.ArgumentParser(description="ç´”ç²¹æ¨¡å‹ API æœå‹™")
    parser.add_argument("--model", 
                       default="Qwen/Qwen2.5-7B-Instruct",
                       help="æ¨¡å‹åç¨± (ä¾‹å¦‚: Qwen/Qwen2.5-7B-Instruct æˆ– meta-llama/Llama-3.1-8B-Instruct)")
    parser.add_argument("--device", 
                       default="auto",
                       help="è£ç½® (auto, cpu, cuda, cuda:0 ç­‰)")
    parser.add_argument("--host", 
                       default="127.0.0.1", 
                       help="API ä¸»æ©Ÿä½å€")
    parser.add_argument("--port", 
                       type=int, 
                       default=5000, 
                       help="API é€£æ¥åŸ ")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–èŠå¤©æ©Ÿå™¨äºº
    print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–ç´”ç²¹æ¨¡å‹ API æœå‹™...")
    print(f"ğŸ“± ä½¿ç”¨æ¨¡å‹: {args.model}")
    print(f"ğŸ”§ è£ç½®: {args.device}")
    
    try:
        chatbot = PureModelChatbot(
            model_name=args.model,
            device=args.device
        )
        
        print(f"âœ… API æœå‹™å•Ÿå‹•æ–¼ http://{args.host}:{args.port}")
        print("ğŸ“‹ å¯ç”¨ç«¯é»:")
        print(f"  POST /chat - å‚³é€è¨Šæ¯")
        print(f"    åƒæ•¸: {{\"message\": \"ä½ çš„è¨Šæ¯\", \"max_tokens\": 512, \"temperature\": 0.7}}")
        print(f"  POST /reset - é‡è¨­å°è©±")
        print(f"  GET /status - å–å¾—ç‹€æ…‹")
        print(f"  GET /health - å¥åº·æª¢æŸ¥")
        
        app.run(host=args.host, port=args.port, debug=False)
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±æ•—: {e}")
        return 1

if __name__ == "__main__":
    main()
